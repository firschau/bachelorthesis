{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4.1. - Supervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this JupyterNotebook, the models described in Chapter 4.1.2. are trained and evaluated, differentiated by calculation approach, classification task, machine learning technique and parameter subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrization-First-Aggregation-Second / Community Profit (PFAS / Profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression (PFAS / Profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features (PFAS / Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 7.7e+01 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.39      0.48      0.43       100\n",
      "         Low       0.37      0.15      0.21       100\n",
      "      Medium       0.38      0.52      0.44       100\n",
      "\n",
      "    accuracy                           0.38       300\n",
      "   macro avg       0.38      0.38      0.36       300\n",
      "weighted avg       0.38      0.38      0.36       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_PFAS_Profit_2020-09-25.pkl')\n",
    "\n",
    "X = data.iloc[:,:-1].copy()\n",
    "y = data['CommunityProfit_Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr_params = {\n",
    "    'penalty' : ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 20, 100],\n",
    "    'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0' : [10e-5, 10e-4, 10e-3, 10e-2, 1],\n",
    "    'tol' : [0.00001, 0.0001, 0.001],\n",
    "    'l1_ratio' : [0.15, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "lr_classifier = SGDClassifier(loss='log',\n",
    "                              random_state=42,\n",
    "                              max_iter=10e+6)\n",
    "\n",
    "lr_gridsearch = GridSearchCV(estimator=lr_classifier,\n",
    "                             param_grid=lr_params,\n",
    "                             scoring='accuracy',\n",
    "                             cv=3,\n",
    "                             n_jobs=3)\n",
    "\n",
    "lr_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "lr_result = pd.DataFrame(lr_gridsearch.cv_results_)\n",
    "lr_result.to_pickle('Data/GridSearchResults_PFAS_Profit_AllFeatures_LR.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=lr_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1, 'eta0': 0.01, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'tol': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_PFAS_Profit_AllFeatures_LR.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selected Features (PFAS / Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 4.0 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.36      0.38      0.37       167\n",
      "         Low       0.39      0.35      0.37       167\n",
      "      Medium       0.31      0.33      0.32       166\n",
      "\n",
      "    accuracy                           0.35       500\n",
      "   macro avg       0.35      0.35      0.35       500\n",
      "weighted avg       0.35      0.35      0.35       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "selected_features = ['median_daily_maximum_demand_winter_weekend', 'night_impact_spring/autumn_weekend', 'fft_peak_summer_workday', 'maximum_tou_winter_weekend', 'maximum_tou_winter_workday', 'maximum_tou_spring/autumn_weekend', 'variance_summer_workday', 'variance_summer_weekend', 'variance_winter_workday', 'variance_winter_weekend', 'end_of_work_impact_spring/autumn_weekend', 'minimum_tou_summer_weekend', 'morning_slope_winter_weekend', 'night_impact_winter_weekend', 'minimum_tou_winter_weekend', 'fft_peak_winter_workday', 'daily_load_factor_winter_weekend', 'fft_peak_winter_weekend', 'median_daily_maximum_demand_winter_workday', 'night_slope_spring/autumn_weekend'] \n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_PFAS_Profit_2020-09-25.pkl')\n",
    "\n",
    "X = data[selected_features]\n",
    "y = data['CommunityProfit_Class']\n",
    "\n",
    "# for col in X.columns:\n",
    "#     X[col] = pd.to_numeric(X[col])\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr_params = {\n",
    "    'penalty' : ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 20, 100],\n",
    "    'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0' : [10e-5, 10e-4, 10e-3, 10e-2, 1],\n",
    "    'tol' : [0.00001, 0.0001, 0.001],\n",
    "    'l1_ratio' : [0.15, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "lr_classifier = SGDClassifier(loss='log',\n",
    "                              random_state=42,\n",
    "                              max_iter=10e+6)\n",
    "\n",
    "lr_gridsearch = GridSearchCV(estimator=lr_classifier,\n",
    "                             param_grid=lr_params,\n",
    "                             scoring='accuracy',\n",
    "                             cv=3,\n",
    "                             n_jobs=3)\n",
    "\n",
    "lr_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "lr_result = pd.DataFrame(lr_gridsearch.cv_results_)\n",
    "lr_result.to_pickle('Data/GridSearchResults_PFAS_Profit_SelectedFeatures_LR.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=lr_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 10, 'eta0': 0.0001, 'l1_ratio': 0.15, 'learning_rate': 'adaptive', 'penalty': 'l2', 'tol': 0.001}\n",
      "{'alpha': 10, 'eta0': 0.0001, 'l1_ratio': 0.3, 'learning_rate': 'adaptive', 'penalty': 'l2', 'tol': 0.001}\n",
      "{'alpha': 10, 'eta0': 0.0001, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l2', 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_PFAS_Profit_SelectedFeatures_LR.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme Gradient Boosting (PFAS / Profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features (PFAS / Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9360 candidates, totalling 28080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=3)]: Done 109 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=3)]: Done 283 tasks      | elapsed:   16.8s\n",
      "[Parallel(n_jobs=3)]: Done 509 tasks      | elapsed:   32.3s\n",
      "[Parallel(n_jobs=3)]: Done 780 tasks      | elapsed:   59.7s\n",
      "[Parallel(n_jobs=3)]: Done 943 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=3)]: Done 1130 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=3)]: Done 1343 tasks      | elapsed: 25.7min\n",
      "[Parallel(n_jobs=3)]: Done 1651 tasks      | elapsed: 34.9min\n",
      "[Parallel(n_jobs=3)]: Done 2177 tasks      | elapsed: 35.7min\n",
      "[Parallel(n_jobs=3)]: Done 2466 tasks      | elapsed: 38.6min\n",
      "[Parallel(n_jobs=3)]: Done 2779 tasks      | elapsed: 66.1min\n",
      "[Parallel(n_jobs=3)]: Done 3319 tasks      | elapsed: 69.7min\n",
      "[Parallel(n_jobs=3)]: Done 3818 tasks      | elapsed: 72.2min\n",
      "[Parallel(n_jobs=3)]: Done 4205 tasks      | elapsed: 104.4min\n",
      "[Parallel(n_jobs=3)]: Done 4910 tasks      | elapsed: 105.7min\n",
      "[Parallel(n_jobs=3)]: Done 5385 tasks      | elapsed: 113.3min\n",
      "[Parallel(n_jobs=3)]: Done 5864 tasks      | elapsed: 140.9min\n",
      "[Parallel(n_jobs=3)]: Done 6617 tasks      | elapsed: 143.5min\n",
      "[Parallel(n_jobs=3)]: Done 7135 tasks      | elapsed: 171.8min\n",
      "[Parallel(n_jobs=3)]: Done 8006 tasks      | elapsed: 174.1min\n",
      "[Parallel(n_jobs=3)]: Done 8609 tasks      | elapsed: 202.3min\n",
      "[Parallel(n_jobs=3)]: Done 9495 tasks      | elapsed: 205.9min\n",
      "[Parallel(n_jobs=3)]: Done 10280 tasks      | elapsed: 233.0min\n",
      "[Parallel(n_jobs=3)]: Done 11081 tasks      | elapsed: 246.1min\n",
      "[Parallel(n_jobs=3)]: Done 12080 tasks      | elapsed: 264.3min\n",
      "[Parallel(n_jobs=3)]: Done 12833 tasks      | elapsed: 286.2min\n",
      "[Parallel(n_jobs=3)]: Done 13840 tasks      | elapsed: 294.4min\n",
      "[Parallel(n_jobs=3)]: Done 14916 tasks      | elapsed: 310.5min\n",
      "[Parallel(n_jobs=3)]: Done 15809 tasks      | elapsed: 333.5min\n",
      "[Parallel(n_jobs=3)]: Done 16802 tasks      | elapsed: 355.0min\n",
      "[Parallel(n_jobs=3)]: Done 17951 tasks      | elapsed: 363.0min\n",
      "[Parallel(n_jobs=3)]: Done 19000 tasks      | elapsed: 379.1min\n",
      "[Parallel(n_jobs=3)]: Done 19991 tasks      | elapsed: 397.4min\n",
      "[Parallel(n_jobs=3)]: Done 21099 tasks      | elapsed: 417.2min\n",
      "[Parallel(n_jobs=3)]: Done 22345 tasks      | elapsed: 428.3min\n",
      "[Parallel(n_jobs=3)]: Done 23693 tasks      | elapsed: 444.5min\n",
      "[Parallel(n_jobs=3)]: Done 24995 tasks      | elapsed: 458.4min\n",
      "[Parallel(n_jobs=3)]: Done 26321 tasks      | elapsed: 473.9min\n",
      "[Parallel(n_jobs=3)]: Done 27727 tasks      | elapsed: 490.8min\n",
      "[Parallel(n_jobs=3)]: Done 28080 out of 28080 | elapsed: 504.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 5e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.38      0.27      0.32       100\n",
      "         Low       0.29      0.32      0.30       100\n",
      "      Medium       0.36      0.42      0.39       100\n",
      "\n",
      "    accuracy                           0.34       300\n",
      "   macro avg       0.34      0.34      0.33       300\n",
      "weighted avg       0.34      0.34      0.33       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_PFAS_Profit_2020-09-25.pkl')\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "y = data['CommunityProfit_Class']\n",
    "\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col])\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_params = {\n",
    "    'reg_alpha' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'reg_lambda' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'learning_rate' : [0, 0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth ' : [3, 5, 7, 9],\n",
    "    'n_estimators' : [10,11,12,13,14,15,20,50,100,200,500,1000,2000]\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax',\n",
    "                                   eval_metric='merror',\n",
    "                                   random_state=42)\n",
    "\n",
    "xgb_gridsearch = GridSearchCV(estimator=xgb_classifier,\n",
    "                              param_grid=xgb_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=3,\n",
    "                              verbose=6)\n",
    "\n",
    "xgb_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "xgb_result = pd.DataFrame(xgb_gridsearch.cv_results_)\n",
    "xgb_result.to_pickle('Data/GridSearchResults_PFAS_Profit_AllFeatures_XGB.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=xgb_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'max_depth ': 3, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 100}\n",
      "{'learning_rate': 0.01, 'max_depth ': 5, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 100}\n",
      "{'learning_rate': 0.01, 'max_depth ': 7, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 100}\n",
      "{'learning_rate': 0.01, 'max_depth ': 9, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 100}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_PFAS_Profit_AllFeatures_XGB.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selected Features (PFAS / Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9360 candidates, totalling 28080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 146 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 398 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=3)]: Done 746 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=3)]: Done 979 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=3)]: Done 1116 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=3)]: Done 1279 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=3)]: Done 1550 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=3)]: Done 2333 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=3)]: Done 2570 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=3)]: Done 2833 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=3)]: Done 3774 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=3)]: Done 4087 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=3)]: Done 4949 tasks      | elapsed: 25.2min\n",
      "[Parallel(n_jobs=3)]: Done 5441 tasks      | elapsed: 27.9min\n",
      "[Parallel(n_jobs=3)]: Done 6362 tasks      | elapsed: 33.5min\n",
      "[Parallel(n_jobs=3)]: Done 6895 tasks      | elapsed: 37.5min\n",
      "[Parallel(n_jobs=3)]: Done 7899 tasks      | elapsed: 43.6min\n",
      "[Parallel(n_jobs=3)]: Done 8362 tasks      | elapsed: 51.1min\n",
      "[Parallel(n_jobs=3)]: Done 9479 tasks      | elapsed: 53.9min\n",
      "[Parallel(n_jobs=3)]: Done 10373 tasks      | elapsed: 61.8min\n",
      "[Parallel(n_jobs=3)]: Done 11165 tasks      | elapsed: 69.0min\n",
      "[Parallel(n_jobs=3)]: Done 12313 tasks      | elapsed: 73.0min\n",
      "[Parallel(n_jobs=3)]: Done 13554 tasks      | elapsed: 79.3min\n",
      "[Parallel(n_jobs=3)]: Done 14437 tasks      | elapsed: 85.6min\n",
      "[Parallel(n_jobs=3)]: Done 15458 tasks      | elapsed: 92.3min\n",
      "[Parallel(n_jobs=3)]: Done 16775 tasks      | elapsed: 97.4min\n",
      "[Parallel(n_jobs=3)]: Done 18116 tasks      | elapsed: 102.1min\n",
      "[Parallel(n_jobs=3)]: Done 19483 tasks      | elapsed: 107.7min\n",
      "[Parallel(n_jobs=3)]: Done 20874 tasks      | elapsed: 113.7min\n",
      "[Parallel(n_jobs=3)]: Done 22369 tasks      | elapsed: 121.4min\n",
      "[Parallel(n_jobs=3)]: Done 23810 tasks      | elapsed: 128.2min\n",
      "[Parallel(n_jobs=3)]: Done 25238 tasks      | elapsed: 134.7min\n",
      "[Parallel(n_jobs=3)]: Done 26672 tasks      | elapsed: 141.1min\n",
      "[Parallel(n_jobs=3)]: Done 28080 out of 28080 | elapsed: 147.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 1.5e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.34      0.26      0.29       100\n",
      "         Low       0.40      0.40      0.40       100\n",
      "      Medium       0.35      0.43      0.38       100\n",
      "\n",
      "    accuracy                           0.36       300\n",
      "   macro avg       0.36      0.36      0.36       300\n",
      "weighted avg       0.36      0.36      0.36       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Import data (and filter) data\n",
    "selected_features = ['median_daily_maximum_demand_winter_weekend', 'night_impact_spring/autumn_weekend', 'fft_peak_summer_workday', 'maximum_tou_winter_weekend', 'maximum_tou_winter_workday', 'maximum_tou_spring/autumn_weekend', 'variance_summer_workday', 'variance_summer_weekend', 'variance_winter_workday', 'variance_winter_weekend', 'end_of_work_impact_spring/autumn_weekend', 'minimum_tou_summer_weekend', 'morning_slope_winter_weekend', 'night_impact_winter_weekend', 'minimum_tou_winter_weekend', 'fft_peak_winter_workday', 'daily_load_factor_winter_weekend', 'fft_peak_winter_weekend', 'median_daily_maximum_demand_winter_workday', 'night_slope_spring/autumn_weekend'] \n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_PFAS_Profit_2020-09-25.pkl')\n",
    "\n",
    "X = data[selected_features]\n",
    "y = data['CommunityProfit_Class']\n",
    "\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col])\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_params = {\n",
    "    'reg_alpha' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'reg_lambda' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'learning_rate' : [0, 0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth ' : [3, 5, 7, 9],\n",
    "    'n_estimators' : [10,11,12,13,14,15,20,50,100,200,500,1000,2000]\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax',\n",
    "                                   eval_metric='merror',\n",
    "                                   random_state=42)\n",
    "\n",
    "xgb_gridsearch = GridSearchCV(estimator=xgb_classifier,\n",
    "                              param_grid=xgb_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=3,\n",
    "                              verbose=6)\n",
    "\n",
    "xgb_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "xgb_result = pd.DataFrame(xgb_gridsearch.cv_results_)\n",
    "xgb_result.to_pickle('Data/GridSearchResults_PFAS_Profit_SelectedFeatures_XGB.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=xgb_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth ': 3, 'n_estimators': 13, 'reg_alpha': 0.001, 'reg_lambda': 1}\n",
      "{'learning_rate': 0.1, 'max_depth ': 5, 'n_estimators': 13, 'reg_alpha': 0.001, 'reg_lambda': 1}\n",
      "{'learning_rate': 0.1, 'max_depth ': 7, 'n_estimators': 13, 'reg_alpha': 0.001, 'reg_lambda': 1}\n",
      "{'learning_rate': 0.1, 'max_depth ': 9, 'n_estimators': 13, 'reg_alpha': 0.001, 'reg_lambda': 1}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_PFAS_Profit_SelectedFeatures_XGB.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron (PFAS / Profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features (PFAS / Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3000 candidates, totalling 9000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done  76 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=3)]: Done 169 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=3)]: Done 332 tasks      | elapsed:   31.1s\n",
      "[Parallel(n_jobs=3)]: Done 493 tasks      | elapsed:   55.2s\n",
      "[Parallel(n_jobs=3)]: Done 654 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=3)]: Done 853 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=3)]: Done 1060 tasks      | elapsed:  4.9min\n",
      "[Parallel(n_jobs=3)]: Done 1397 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=3)]: Done 1660 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=3)]: Done 1975 tasks      | elapsed:  9.6min\n",
      "[Parallel(n_jobs=3)]: Done 2331 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=3)]: Done 2644 tasks      | elapsed: 19.3min\n",
      "[Parallel(n_jobs=3)]: Done 3044 tasks      | elapsed: 23.9min\n",
      "[Parallel(n_jobs=3)]: Done 3455 tasks      | elapsed: 28.1min\n",
      "[Parallel(n_jobs=3)]: Done 3842 tasks      | elapsed: 37.4min\n",
      "[Parallel(n_jobs=3)]: Done 4255 tasks      | elapsed: 45.9min\n",
      "[Parallel(n_jobs=3)]: Done 4692 tasks      | elapsed: 56.3min\n",
      "[Parallel(n_jobs=3)]: Done 5155 tasks      | elapsed: 64.9min\n",
      "[Parallel(n_jobs=3)]: Done 5642 tasks      | elapsed: 77.2min\n",
      "[Parallel(n_jobs=3)]: Done 6155 tasks      | elapsed: 101.6min\n",
      "[Parallel(n_jobs=3)]: Done 6692 tasks      | elapsed: 132.0min\n",
      "[Parallel(n_jobs=3)]: Done 7255 tasks      | elapsed: 176.9min\n",
      "[Parallel(n_jobs=3)]: Done 7842 tasks      | elapsed: 208.2min\n",
      "[Parallel(n_jobs=3)]: Done 8455 tasks      | elapsed: 263.7min\n",
      "[Parallel(n_jobs=3)]: Done 9000 out of 9000 | elapsed: 336.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 3.4e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.38      0.35      0.37       100\n",
      "         Low       0.35      0.38      0.36       100\n",
      "      Medium       0.37      0.37      0.37       100\n",
      "\n",
      "    accuracy                           0.37       300\n",
      "   macro avg       0.37      0.37      0.37       300\n",
      "weighted avg       0.37      0.37      0.37       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_PFAS_Profit_2020-09-25.pkl')\n",
    "\n",
    "X = StandardScaler().fit_transform(data.iloc[:,:-1])\n",
    "y = data['CommunityProfit_Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "hls = []\n",
    "for i in [10, 30, 50, 70, 100]:\n",
    "    for j in [1, 2, 3, 4 , 5, 10, 30, 50, 70, 100]:\n",
    "        hls.append((i,)*j)\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes' : hls,\n",
    "    'solver' : ['adam', 'lbfgs'],\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'beta_1' : [0.8, 0.9],\n",
    "    'tol' : [0.000001, 0.00001, 0.0001]\n",
    "}\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42,\n",
    "                               max_iter=10e+8,\n",
    "                               activation='relu')\n",
    "\n",
    "mlp_gridsearch = GridSearchCV(estimator=mlp_classifier,\n",
    "                              param_grid=mlp_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=3,\n",
    "                              verbose=6)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mlp_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "mlp_result = pd.DataFrame(mlp_gridsearch.cv_results_)\n",
    "mlp_result.to_pickle('Data/GridSearchResults_PFAS_Profit_AllFeatures_MLP.pkl')\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=mlp_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001, 'beta_1': 0.8, 'hidden_layer_sizes': (30, 30, 30), 'solver': 'lbfgs', 'tol': 1e-06}\n",
      "{'alpha': 0.0001, 'beta_1': 0.9, 'hidden_layer_sizes': (30, 30, 30), 'solver': 'lbfgs', 'tol': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_PFAS_Profit_AllFeatures_MLP.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selected Features (PFAS / Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3000 candidates, totalling 9000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done  68 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=3)]: Done 131 tasks      | elapsed:   41.2s\n",
      "[Parallel(n_jobs=3)]: Done 294 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done 440 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=3)]: Done 579 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=3)]: Done 742 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=3)]: Done 929 tasks      | elapsed:  7.3min\n",
      "[Parallel(n_jobs=3)]: Done 1214 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=3)]: Done 1508 tasks      | elapsed: 10.0min\n",
      "[Parallel(n_jobs=3)]: Done 1771 tasks      | elapsed: 13.9min\n",
      "[Parallel(n_jobs=3)]: Done 2085 tasks      | elapsed: 16.2min\n",
      "[Parallel(n_jobs=3)]: Done 2398 tasks      | elapsed: 21.0min\n",
      "[Parallel(n_jobs=3)]: Done 2735 tasks      | elapsed: 37.1min\n",
      "[Parallel(n_jobs=3)]: Done 3149 tasks      | elapsed: 39.6min\n",
      "[Parallel(n_jobs=3)]: Done 3536 tasks      | elapsed: 49.0min\n",
      "[Parallel(n_jobs=3)]: Done 3964 tasks      | elapsed: 61.6min\n",
      "[Parallel(n_jobs=3)]: Done 4401 tasks      | elapsed: 76.4min\n",
      "[Parallel(n_jobs=3)]: Done 4897 tasks      | elapsed: 92.3min\n",
      "[Parallel(n_jobs=3)]: Done 5384 tasks      | elapsed: 116.6min\n",
      "[Parallel(n_jobs=3)]: Done 5897 tasks      | elapsed: 137.5min\n",
      "[Parallel(n_jobs=3)]: Done 6434 tasks      | elapsed: 185.0min\n",
      "[Parallel(n_jobs=3)]: Done 6997 tasks      | elapsed: 216.6min\n",
      "[Parallel(n_jobs=3)]: Done 7593 tasks      | elapsed: 260.0min\n",
      "[Parallel(n_jobs=3)]: Done 8206 tasks      | elapsed: 327.2min\n",
      "[Parallel(n_jobs=3)]: Done 8852 tasks      | elapsed: 379.7min\n",
      "[Parallel(n_jobs=3)]: Done 9000 out of 9000 | elapsed: 407.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 4.1e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.36      0.32      0.34       100\n",
      "         Low       0.36      0.36      0.36       100\n",
      "      Medium       0.35      0.38      0.36       100\n",
      "\n",
      "    accuracy                           0.35       300\n",
      "   macro avg       0.35      0.35      0.35       300\n",
      "weighted avg       0.35      0.35      0.35       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_PFAS_Profit_2020-09-25.pkl')\n",
    "\n",
    "selected_features = ['median_daily_maximum_demand_winter_weekend', 'night_impact_spring/autumn_weekend', 'fft_peak_summer_workday', 'maximum_tou_winter_weekend', 'maximum_tou_winter_workday', 'maximum_tou_spring/autumn_weekend', 'variance_summer_workday', 'variance_summer_weekend', 'variance_winter_workday', 'variance_winter_weekend', 'end_of_work_impact_spring/autumn_weekend', 'minimum_tou_summer_weekend', 'morning_slope_winter_weekend', 'night_impact_winter_weekend', 'minimum_tou_winter_weekend', 'fft_peak_winter_workday', 'daily_load_factor_winter_weekend', 'fft_peak_winter_weekend', 'median_daily_maximum_demand_winter_workday', 'night_slope_spring/autumn_weekend'] \n",
    "\n",
    "X = StandardScaler().fit_transform(data[selected_features])\n",
    "y = data['CommunityProfit_Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "hls = []\n",
    "for i in [10, 30, 50, 70, 100]:\n",
    "    for j in [1, 2, 3, 4 , 5, 10, 30, 50, 70, 100]:\n",
    "        hls.append((i,)*j)\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes' : hls,\n",
    "    'solver' : ['adam', 'lbfgs'],\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'beta_1' : [0.8, 0.9],\n",
    "    'tol' : [0.000001, 0.00001, 0.0001]\n",
    "}\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42,\n",
    "                               max_iter=10e+8,\n",
    "                               activation='relu')\n",
    "\n",
    "mlp_gridsearch = GridSearchCV(estimator=mlp_classifier,\n",
    "                              param_grid=mlp_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=3,\n",
    "                              verbose=6)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mlp_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "mlp_result = pd.DataFrame(mlp_gridsearch.cv_results_)\n",
    "mlp_result.to_pickle('Data/GridSearchResults_PFAS_Profit_SelectedFeatures_MLP.pkl')\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=mlp_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1, 'beta_1': 0.8, 'hidden_layer_sizes': (50, 50, 50, 50, 50), 'solver': 'lbfgs', 'tol': 0.0001}\n",
      "{'alpha': 0.1, 'beta_1': 0.9, 'hidden_layer_sizes': (50, 50, 50, 50, 50), 'solver': 'lbfgs', 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_PFAS_Profit_SelectedFeatures_MLP.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrization-First-Aggregation-Second / Community Gain (PFAS / Gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression (PFAS / Gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features (PFAS / Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 3.5e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.33      0.30      0.31       100\n",
      "         Low       0.35      0.33      0.34       100\n",
      "      Medium       0.38      0.44      0.41       100\n",
      "\n",
      "    accuracy                           0.36       300\n",
      "   macro avg       0.35      0.36      0.35       300\n",
      "weighted avg       0.35      0.36      0.35       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_PFAS_Gain_2020-09-25.pkl')\n",
    "\n",
    "X = data.iloc[:,:-1].copy()\n",
    "y = data['CommunityGain_Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr_params = {\n",
    "    'penalty' : ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 20, 100],\n",
    "    'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0' : [10e-5, 10e-4, 10e-3, 10e-2, 1],\n",
    "    'tol' : [0.00001, 0.0001, 0.001],\n",
    "    'l1_ratio' : [0.15, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "lr_classifier = SGDClassifier(loss='log',\n",
    "                              random_state=42,\n",
    "                              max_iter=10e+6)\n",
    "\n",
    "lr_gridsearch = GridSearchCV(estimator=lr_classifier,\n",
    "                             param_grid=lr_params,\n",
    "                             scoring='accuracy',\n",
    "                             cv=3,\n",
    "                             n_jobs=3)\n",
    "\n",
    "lr_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "lr_result = pd.DataFrame(lr_gridsearch.cv_results_)\n",
    "lr_result.to_pickle('Data/GridSearchResults_PFAS_Gain_AllFeatures_LR.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=lr_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.01, 'eta0': 0.01, 'l1_ratio': 0.15, 'learning_rate': 'adaptive', 'penalty': 'l1', 'tol': 0.001}\n",
      "{'alpha': 0.01, 'eta0': 0.01, 'l1_ratio': 0.3, 'learning_rate': 'adaptive', 'penalty': 'l1', 'tol': 0.001}\n",
      "{'alpha': 0.01, 'eta0': 0.01, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l1', 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_PFAS_Gain_AllFeatures_LR.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selected Features (PFAS / Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 5.4 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.33      0.41      0.36       167\n",
      "         Low       0.39      0.29      0.33       167\n",
      "      Medium       0.38      0.39      0.38       166\n",
      "\n",
      "    accuracy                           0.36       500\n",
      "   macro avg       0.37      0.36      0.36       500\n",
      "weighted avg       0.37      0.36      0.36       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "selected_features = ['maximum_tou_summer_workday', 'end_of_work_impact_summer_weekend', 'lunch_impact_spring/autumn_weekend', 'minimum_tou_spring/autumn_workday', 'kurtosis_summer_workday', 'kurtosis_summer_weekend', 'fft_peak_summer_weekend', 'morning_slope_summer_workday', 'end_of_work_impact_summer_workday', 'median_daily_maximum_demand_spring/autumn_workday', 'night_impact_winter_weekend', 'morning_slope_spring/autumn_workday', 'fft_peak_summer_workday', 'minimum_tou_spring/autumn_weekend', 'minimum_tou_winter_weekend', 'minimum_tou_winter_workday', 'median_daily_minimum_demand_summer_workday', 'variance_spring/autumn_workday', 'night_slope_winter_weekend', 'fft_peak_winter_workday'] \n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_PFAS_Gain_2020-09-25.pkl')\n",
    "\n",
    "X = data[selected_features]\n",
    "y = data['CommunityGain_Class']\n",
    "\n",
    "# for col in X.columns:\n",
    "#     X[col] = pd.to_numeric(X[col])\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr_params = {\n",
    "    'penalty' : ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 20, 100],\n",
    "    'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0' : [10e-5, 10e-4, 10e-3, 10e-2, 1],\n",
    "    'tol' : [0.00001, 0.0001, 0.001],\n",
    "    'l1_ratio' : [0.15, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "lr_classifier = SGDClassifier(loss='log',\n",
    "                              random_state=42,\n",
    "                              max_iter=10e+6)\n",
    "\n",
    "lr_gridsearch = GridSearchCV(estimator=lr_classifier,\n",
    "                             param_grid=lr_params,\n",
    "                             scoring='accuracy',\n",
    "                             cv=3,\n",
    "                             n_jobs=3)\n",
    "\n",
    "lr_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "lr_result = pd.DataFrame(lr_gridsearch.cv_results_)\n",
    "lr_result.to_pickle('Data/GridSearchResults_PFAS_Gain_SelectedFeatures_LR.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=lr_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.0001, 'eta0': 0.1, 'l1_ratio': 0.15, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'tol': 1e-05}\n",
      "{'alpha': 0.0001, 'eta0': 0.1, 'l1_ratio': 0.15, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'tol': 0.0001}\n",
      "{'alpha': 0.0001, 'eta0': 0.1, 'l1_ratio': 0.15, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_PFAS_Gain_SelectedFeatures_LR.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme Gradient Boosting (PFAS / Gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features (PFAS / Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9360 candidates, totalling 28080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=3)]: Done 109 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=3)]: Done 283 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=3)]: Done 509 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=3)]: Done 782 tasks      | elapsed:   59.7s\n",
      "[Parallel(n_jobs=3)]: Done 946 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=3)]: Done 1133 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=3)]: Done 1346 tasks      | elapsed: 24.9min\n",
      "[Parallel(n_jobs=3)]: Done 1724 tasks      | elapsed: 33.0min\n",
      "[Parallel(n_jobs=3)]: Done 2215 tasks      | elapsed: 33.8min\n",
      "[Parallel(n_jobs=3)]: Done 2502 tasks      | elapsed: 37.3min\n",
      "[Parallel(n_jobs=3)]: Done 2815 tasks      | elapsed: 65.5min\n",
      "[Parallel(n_jobs=3)]: Done 3391 tasks      | elapsed: 66.1min\n",
      "[Parallel(n_jobs=3)]: Done 3854 tasks      | elapsed: 68.9min\n",
      "[Parallel(n_jobs=3)]: Done 4241 tasks      | elapsed: 98.2min\n",
      "[Parallel(n_jobs=3)]: Done 4991 tasks      | elapsed: 99.1min\n",
      "[Parallel(n_jobs=3)]: Done 5430 tasks      | elapsed: 108.6min\n",
      "[Parallel(n_jobs=3)]: Done 6065 tasks      | elapsed: 131.3min\n",
      "[Parallel(n_jobs=3)]: Done 6716 tasks      | elapsed: 135.6min\n",
      "[Parallel(n_jobs=3)]: Done 7333 tasks      | elapsed: 161.6min\n",
      "[Parallel(n_jobs=3)]: Done 8102 tasks      | elapsed: 165.2min\n",
      "[Parallel(n_jobs=3)]: Done 8795 tasks      | elapsed: 192.0min\n",
      "[Parallel(n_jobs=3)]: Done 9585 tasks      | elapsed: 199.0min\n",
      "[Parallel(n_jobs=3)]: Done 10460 tasks      | elapsed: 222.8min\n",
      "[Parallel(n_jobs=3)]: Done 11171 tasks      | elapsed: 245.8min\n",
      "[Parallel(n_jobs=3)]: Done 12170 tasks      | elapsed: 254.4min\n",
      "[Parallel(n_jobs=3)]: Done 13088 tasks      | elapsed: 275.9min\n",
      "[Parallel(n_jobs=3)]: Done 13918 tasks      | elapsed: 288.2min\n",
      "[Parallel(n_jobs=3)]: Done 14994 tasks      | elapsed: 300.7min\n",
      "[Parallel(n_jobs=3)]: Done 15965 tasks      | elapsed: 322.2min\n",
      "[Parallel(n_jobs=3)]: Done 16880 tasks      | elapsed: 344.7min\n",
      "[Parallel(n_jobs=3)]: Done 18032 tasks      | elapsed: 350.6min\n",
      "[Parallel(n_jobs=3)]: Done 19208 tasks      | elapsed: 365.1min\n",
      "[Parallel(n_jobs=3)]: Done 20455 tasks      | elapsed: 382.8min\n",
      "[Parallel(n_jobs=3)]: Done 21519 tasks      | elapsed: 400.7min\n",
      "[Parallel(n_jobs=3)]: Done 22619 tasks      | elapsed: 419.0min\n",
      "[Parallel(n_jobs=3)]: Done 23870 tasks      | elapsed: 435.6min\n",
      "[Parallel(n_jobs=3)]: Done 25253 tasks      | elapsed: 451.4min\n",
      "[Parallel(n_jobs=3)]: Done 26579 tasks      | elapsed: 462.6min\n",
      "[Parallel(n_jobs=3)]: Done 27931 tasks      | elapsed: 477.7min\n",
      "[Parallel(n_jobs=3)]: Done 28080 out of 28080 | elapsed: 486.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 4.9e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.29      0.27      0.28       100\n",
      "         Low       0.32      0.39      0.35       100\n",
      "      Medium       0.23      0.19      0.21       100\n",
      "\n",
      "    accuracy                           0.28       300\n",
      "   macro avg       0.28      0.28      0.28       300\n",
      "weighted avg       0.28      0.28      0.28       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_PFAS_Gain_2020-09-25.pkl')\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "y = data['CommunityGain_Class']\n",
    "\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col])\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_params = {\n",
    "    'reg_alpha' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'reg_lambda' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'learning_rate' : [0, 0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth ' : [3, 5, 7, 9],\n",
    "    'n_estimators' : [10,11,12,13,14,15,20,50,100,200,500,1000,2000]\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax',\n",
    "                                   eval_metric='merror',\n",
    "                                   random_state=42)\n",
    "\n",
    "xgb_gridsearch = GridSearchCV(estimator=xgb_classifier,\n",
    "                              param_grid=xgb_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=3,\n",
    "                              verbose=6)\n",
    "\n",
    "xgb_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "xgb_result = pd.DataFrame(xgb_gridsearch.cv_results_)\n",
    "xgb_result.to_pickle('Data/GridSearchResults_PFAS_Gain_AllFeatures_XGB.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=xgb_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'max_depth ': 3, 'n_estimators': 13, 'reg_alpha': 0.1, 'reg_lambda': 10}\n",
      "{'learning_rate': 0.01, 'max_depth ': 3, 'n_estimators': 50, 'reg_alpha': 1, 'reg_lambda': 10}\n",
      "{'learning_rate': 0.01, 'max_depth ': 5, 'n_estimators': 13, 'reg_alpha': 0.1, 'reg_lambda': 10}\n",
      "{'learning_rate': 0.01, 'max_depth ': 5, 'n_estimators': 50, 'reg_alpha': 1, 'reg_lambda': 10}\n",
      "{'learning_rate': 0.01, 'max_depth ': 7, 'n_estimators': 13, 'reg_alpha': 0.1, 'reg_lambda': 10}\n",
      "{'learning_rate': 0.01, 'max_depth ': 7, 'n_estimators': 50, 'reg_alpha': 1, 'reg_lambda': 10}\n",
      "{'learning_rate': 0.01, 'max_depth ': 9, 'n_estimators': 13, 'reg_alpha': 0.1, 'reg_lambda': 10}\n",
      "{'learning_rate': 0.01, 'max_depth ': 9, 'n_estimators': 50, 'reg_alpha': 1, 'reg_lambda': 10}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_PFAS_Gain_AllFeatures_XGB.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selected Features (PFAS / Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9360 candidates, totalling 28080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 146 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=3)]: Done 398 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=3)]: Done 746 tasks      | elapsed:   15.1s\n",
      "[Parallel(n_jobs=3)]: Done 979 tasks      | elapsed:   37.2s\n",
      "[Parallel(n_jobs=3)]: Done 1116 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=3)]: Done 1279 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=3)]: Done 1514 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=3)]: Done 2324 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=3)]: Done 2561 tasks      | elapsed: 10.3min\n",
      "[Parallel(n_jobs=3)]: Done 2824 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=3)]: Done 3747 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=3)]: Done 4060 tasks      | elapsed: 20.3min\n",
      "[Parallel(n_jobs=3)]: Done 4805 tasks      | elapsed: 25.3min\n",
      "[Parallel(n_jobs=3)]: Done 5417 tasks      | elapsed: 27.6min\n",
      "[Parallel(n_jobs=3)]: Done 6257 tasks      | elapsed: 33.7min\n",
      "[Parallel(n_jobs=3)]: Done 6871 tasks      | elapsed: 37.1min\n",
      "[Parallel(n_jobs=3)]: Done 7962 tasks      | elapsed: 42.0min\n",
      "[Parallel(n_jobs=3)]: Done 8425 tasks      | elapsed: 49.3min\n",
      "[Parallel(n_jobs=3)]: Done 9566 tasks      | elapsed: 51.1min\n",
      "[Parallel(n_jobs=3)]: Done 10703 tasks      | elapsed: 58.0min\n",
      "[Parallel(n_jobs=3)]: Done 11264 tasks      | elapsed: 66.3min\n",
      "[Parallel(n_jobs=3)]: Done 12469 tasks      | elapsed: 69.6min\n",
      "[Parallel(n_jobs=3)]: Done 13698 tasks      | elapsed: 74.7min\n",
      "[Parallel(n_jobs=3)]: Done 14915 tasks      | elapsed: 81.4min\n",
      "[Parallel(n_jobs=3)]: Done 16010 tasks      | elapsed: 88.3min\n",
      "[Parallel(n_jobs=3)]: Done 16991 tasks      | elapsed: 95.5min\n",
      "[Parallel(n_jobs=3)]: Done 18248 tasks      | elapsed: 101.2min\n",
      "[Parallel(n_jobs=3)]: Done 19615 tasks      | elapsed: 106.2min\n",
      "[Parallel(n_jobs=3)]: Done 21006 tasks      | elapsed: 111.7min\n",
      "[Parallel(n_jobs=3)]: Done 22408 tasks      | elapsed: 117.8min\n",
      "[Parallel(n_jobs=3)]: Done 23840 tasks      | elapsed: 124.1min\n",
      "[Parallel(n_jobs=3)]: Done 25313 tasks      | elapsed: 129.8min\n",
      "[Parallel(n_jobs=3)]: Done 27062 tasks      | elapsed: 135.5min\n",
      "[Parallel(n_jobs=3)]: Done 28080 out of 28080 | elapsed: 140.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 1.4e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.27      0.19      0.22       100\n",
      "         Low       0.23      0.24      0.23       100\n",
      "      Medium       0.23      0.29      0.26       100\n",
      "\n",
      "    accuracy                           0.24       300\n",
      "   macro avg       0.24      0.24      0.24       300\n",
      "weighted avg       0.24      0.24      0.24       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Import data (and filter) data\n",
    "selected_features = ['maximum_tou_summer_workday', 'end_of_work_impact_summer_weekend', 'lunch_impact_spring/autumn_weekend', 'minimum_tou_spring/autumn_workday', 'kurtosis_summer_workday', 'kurtosis_summer_weekend', 'fft_peak_summer_weekend', 'morning_slope_summer_workday', 'end_of_work_impact_summer_workday', 'median_daily_maximum_demand_spring/autumn_workday', 'night_impact_winter_weekend', 'morning_slope_spring/autumn_workday', 'fft_peak_summer_workday', 'minimum_tou_spring/autumn_weekend', 'minimum_tou_winter_weekend', 'minimum_tou_winter_workday', 'median_daily_minimum_demand_summer_workday', 'variance_spring/autumn_workday', 'night_slope_winter_weekend', 'fft_peak_winter_workday'] \n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_PFAS_Gain_2020-09-25.pkl')\n",
    "\n",
    "X = data[selected_features]\n",
    "y = data['CommunityGain_Class']\n",
    "\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col])\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_params = {\n",
    "    'reg_alpha' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'reg_lambda' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'learning_rate' : [0, 0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth ' : [3, 5, 7, 9],\n",
    "    'n_estimators' : [10,11,12,13,14,15,20,50,100,200,500,1000,2000]\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax',\n",
    "                                   eval_metric='merror',\n",
    "                                   random_state=42)\n",
    "\n",
    "xgb_gridsearch = GridSearchCV(estimator=xgb_classifier,\n",
    "                              param_grid=xgb_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=3,\n",
    "                              verbose=6)\n",
    "\n",
    "xgb_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "xgb_result = pd.DataFrame(xgb_gridsearch.cv_results_)\n",
    "xgb_result.to_pickle('Data/GridSearchResults_PFAS_Gain_SelectedFeatures_XGB.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=xgb_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'max_depth ': 3, 'n_estimators': 11, 'reg_alpha': 1, 'reg_lambda': 0}\n",
      "{'learning_rate': 0.01, 'max_depth ': 5, 'n_estimators': 11, 'reg_alpha': 1, 'reg_lambda': 0}\n",
      "{'learning_rate': 0.01, 'max_depth ': 7, 'n_estimators': 11, 'reg_alpha': 1, 'reg_lambda': 0}\n",
      "{'learning_rate': 0.01, 'max_depth ': 9, 'n_estimators': 11, 'reg_alpha': 1, 'reg_lambda': 0}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_PFAS_Gain_SelectedFeatures_XGB.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron (PFAS / Gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features (PFAS / Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3000 candidates, totalling 9000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done  95 tasks      | elapsed:   11.1s\n",
      "[Parallel(n_jobs=3)]: Done 178 tasks      | elapsed:   27.4s\n",
      "[Parallel(n_jobs=3)]: Done 340 tasks      | elapsed:   50.6s\n",
      "[Parallel(n_jobs=3)]: Done 501 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=3)]: Done 678 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=3)]: Done 841 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=3)]: Done 1052 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=3)]: Done 1389 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=3)]: Done 1664 tasks      | elapsed:  7.0min\n",
      "[Parallel(n_jobs=3)]: Done 1927 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=3)]: Done 2301 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=3)]: Done 2614 tasks      | elapsed: 15.0min\n",
      "[Parallel(n_jobs=3)]: Done 3020 tasks      | elapsed: 21.6min\n",
      "[Parallel(n_jobs=3)]: Done 3410 tasks      | elapsed: 26.1min\n",
      "[Parallel(n_jobs=3)]: Done 3806 tasks      | elapsed: 33.3min\n",
      "[Parallel(n_jobs=3)]: Done 4219 tasks      | elapsed: 38.8min\n",
      "[Parallel(n_jobs=3)]: Done 4663 tasks      | elapsed: 53.7min\n",
      "[Parallel(n_jobs=3)]: Done 5128 tasks      | elapsed: 59.3min\n",
      "[Parallel(n_jobs=3)]: Done 5615 tasks      | elapsed: 74.6min\n",
      "[Parallel(n_jobs=3)]: Done 6128 tasks      | elapsed: 97.4min\n",
      "[Parallel(n_jobs=3)]: Done 6665 tasks      | elapsed: 122.9min\n",
      "[Parallel(n_jobs=3)]: Done 7228 tasks      | elapsed: 161.0min\n",
      "[Parallel(n_jobs=3)]: Done 7824 tasks      | elapsed: 202.7min\n",
      "[Parallel(n_jobs=3)]: Done 8437 tasks      | elapsed: 271.7min\n",
      "[Parallel(n_jobs=3)]: Done 9000 out of 9000 | elapsed: 353.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 3.5e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.39      0.39      0.39       100\n",
      "         Low       0.27      0.28      0.28       100\n",
      "      Medium       0.27      0.26      0.26       100\n",
      "\n",
      "    accuracy                           0.31       300\n",
      "   macro avg       0.31      0.31      0.31       300\n",
      "weighted avg       0.31      0.31      0.31       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_PFAS_Gain_2020-09-25.pkl')\n",
    "\n",
    "X = StandardScaler().fit_transform(data.iloc[:,:-1])\n",
    "y = data['CommunityGain_Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "hls = []\n",
    "for i in [10, 30, 50, 70, 100]:\n",
    "    for j in [1, 2, 3, 4 , 5, 10, 30, 50, 70, 100]:\n",
    "        hls.append((i,)*j)\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes' : hls,\n",
    "    'solver' : ['adam', 'lbfgs'],\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'beta_1' : [0.8, 0.9],\n",
    "    'tol' : [0.000001, 0.00001, 0.0001]\n",
    "}\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42,\n",
    "                               max_iter=10e+8,\n",
    "                               activation='relu')\n",
    "\n",
    "mlp_gridsearch = GridSearchCV(estimator=mlp_classifier,\n",
    "                              param_grid=mlp_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=3,\n",
    "                              verbose=6)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mlp_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "mlp_result = pd.DataFrame(mlp_gridsearch.cv_results_)\n",
    "mlp_result.to_pickle('Data/GridSearchResults_PFAS_Gain_AllFeatures_MLP.pkl')\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=mlp_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1, 'beta_1': 0.8, 'hidden_layer_sizes': (70, 70, 70, 70), 'solver': 'lbfgs', 'tol': 1e-06}\n",
      "{'alpha': 1, 'beta_1': 0.8, 'hidden_layer_sizes': (70, 70, 70, 70), 'solver': 'lbfgs', 'tol': 1e-05}\n",
      "{'alpha': 1, 'beta_1': 0.8, 'hidden_layer_sizes': (70, 70, 70, 70), 'solver': 'lbfgs', 'tol': 0.0001}\n",
      "{'alpha': 1, 'beta_1': 0.9, 'hidden_layer_sizes': (70, 70, 70, 70), 'solver': 'lbfgs', 'tol': 1e-06}\n",
      "{'alpha': 1, 'beta_1': 0.9, 'hidden_layer_sizes': (70, 70, 70, 70), 'solver': 'lbfgs', 'tol': 1e-05}\n",
      "{'alpha': 1, 'beta_1': 0.9, 'hidden_layer_sizes': (70, 70, 70, 70), 'solver': 'lbfgs', 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_PFAS_Gain_AllFeatures_MLP.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selected Features (PFAS / Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3000 candidates, totalling 9000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done  68 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=3)]: Done 131 tasks      | elapsed:   57.5s\n",
      "[Parallel(n_jobs=3)]: Done 294 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=3)]: Done 478 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=3)]: Done 642 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=3)]: Done 805 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=3)]: Done 992 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=3)]: Done 1301 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=3)]: Done 1622 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=3)]: Done 1885 tasks      | elapsed: 23.1min\n",
      "[Parallel(n_jobs=3)]: Done 2280 tasks      | elapsed: 31.0min\n",
      "[Parallel(n_jobs=3)]: Done 2593 tasks      | elapsed: 35.2min\n",
      "[Parallel(n_jobs=3)]: Done 2970 tasks      | elapsed: 45.8min\n",
      "[Parallel(n_jobs=3)]: Done 3362 tasks      | elapsed: 52.1min\n",
      "[Parallel(n_jobs=3)]: Done 3749 tasks      | elapsed: 65.7min\n",
      "[Parallel(n_jobs=3)]: Done 4171 tasks      | elapsed: 74.4min\n",
      "[Parallel(n_jobs=3)]: Done 4608 tasks      | elapsed: 92.3min\n",
      "[Parallel(n_jobs=3)]: Done 5080 tasks      | elapsed: 100.3min\n",
      "[Parallel(n_jobs=3)]: Done 5567 tasks      | elapsed: 118.1min\n",
      "[Parallel(n_jobs=3)]: Done 6080 tasks      | elapsed: 150.3min\n",
      "[Parallel(n_jobs=3)]: Done 6617 tasks      | elapsed: 190.4min\n",
      "[Parallel(n_jobs=3)]: Done 7180 tasks      | elapsed: 247.0min\n",
      "[Parallel(n_jobs=3)]: Done 7767 tasks      | elapsed: 283.9min\n",
      "[Parallel(n_jobs=3)]: Done 8380 tasks      | elapsed: 338.3min\n",
      "[Parallel(n_jobs=3)]: Done 9000 out of 9000 | elapsed: 404.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 4e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.38      0.45      0.41       100\n",
      "         Low       0.30      0.25      0.27       100\n",
      "      Medium       0.29      0.29      0.29       100\n",
      "\n",
      "    accuracy                           0.33       300\n",
      "   macro avg       0.33      0.33      0.33       300\n",
      "weighted avg       0.33      0.33      0.33       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_PFAS_Gain_2020-09-25.pkl')\n",
    "\n",
    "selected_features = ['maximum_tou_summer_workday', 'end_of_work_impact_summer_weekend', 'lunch_impact_spring/autumn_weekend', 'minimum_tou_spring/autumn_workday', 'kurtosis_summer_workday', 'kurtosis_summer_weekend', 'fft_peak_summer_weekend', 'morning_slope_summer_workday', 'end_of_work_impact_summer_workday', 'median_daily_maximum_demand_spring/autumn_workday', 'night_impact_winter_weekend', 'morning_slope_spring/autumn_workday', 'fft_peak_summer_workday', 'minimum_tou_spring/autumn_weekend', 'minimum_tou_winter_weekend', 'minimum_tou_winter_workday', 'median_daily_minimum_demand_summer_workday', 'variance_spring/autumn_workday', 'night_slope_winter_weekend', 'fft_peak_winter_workday'] \n",
    "\n",
    "X = StandardScaler().fit_transform(data[selected_features])\n",
    "y = data['CommunityGain_Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "hls = []\n",
    "for i in [10, 30, 50, 70, 100]:\n",
    "    for j in [1, 2, 3, 4 , 5, 10, 30, 50, 70, 100]:\n",
    "        hls.append((i,)*j)\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes' : hls,\n",
    "    'solver' : ['adam', 'lbfgs'],\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'beta_1' : [0.8, 0.9],\n",
    "    'tol' : [0.000001, 0.00001, 0.0001]\n",
    "}\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42,\n",
    "                               max_iter=10e+8,\n",
    "                               activation='relu')\n",
    "\n",
    "mlp_gridsearch = GridSearchCV(estimator=mlp_classifier,\n",
    "                              param_grid=mlp_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=3,\n",
    "                              verbose=6)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mlp_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "mlp_result = pd.DataFrame(mlp_gridsearch.cv_results_)\n",
    "mlp_result.to_pickle('Data/GridSearchResults_PFAS_Gain_SelectedFeatures_MLP.pkl')\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=mlp_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.01, 'beta_1': 0.8, 'hidden_layer_sizes': (30, 30, 30, 30, 30), 'solver': 'lbfgs', 'tol': 1e-05}\n",
      "{'alpha': 0.01, 'beta_1': 0.9, 'hidden_layer_sizes': (30, 30, 30, 30, 30), 'solver': 'lbfgs', 'tol': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_PFAS_Gain_SelectedFeatures_MLP.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation-First-Parametrization-Second / Community Profit (AFPS / Profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression (AFPS / Profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features (AFPS / Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 5.9e+01 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.38      0.44      0.41       100\n",
      "         Low       0.30      0.21      0.25       100\n",
      "      Medium       0.36      0.41      0.38       100\n",
      "\n",
      "    accuracy                           0.35       300\n",
      "   macro avg       0.35      0.35      0.35       300\n",
      "weighted avg       0.35      0.35      0.35       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_AFPS_Profit_2020-09-25.pkl')\n",
    "\n",
    "X = data.iloc[:,:-1].copy()\n",
    "y = data['CommunityProfit_Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr_params = {\n",
    "    'penalty' : ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 20, 100],\n",
    "    'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0' : [10e-5, 10e-4, 10e-3, 10e-2, 1],\n",
    "    'tol' : [0.00001, 0.0001, 0.001],\n",
    "    'l1_ratio' : [0.15, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "lr_classifier = SGDClassifier(loss='log',\n",
    "                              random_state=42,\n",
    "                              max_iter=10e+6)\n",
    "\n",
    "lr_gridsearch = GridSearchCV(estimator=lr_classifier,\n",
    "                             param_grid=lr_params,\n",
    "                             scoring='accuracy',\n",
    "                             cv=3,\n",
    "                             n_jobs=3)\n",
    "\n",
    "lr_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "lr_result = pd.DataFrame(lr_gridsearch.cv_results_)\n",
    "lr_result.to_pickle('Data/GridSearchResults_AFPS_Profit_AllFeatures_LR.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=lr_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1, 'eta0': 1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'tol': 1e-05}\n",
      "{'alpha': 0.1, 'eta0': 1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_AFPS_Profit_AllFeatures_LR.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selected Features (AFPS / Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 2.0 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.37      0.53      0.43       167\n",
      "         Low       0.38      0.16      0.23       167\n",
      "      Medium       0.37      0.42      0.39       166\n",
      "\n",
      "    accuracy                           0.37       500\n",
      "   macro avg       0.37      0.37      0.35       500\n",
      "weighted avg       0.37      0.37      0.35       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "selected_features =  ['median_daily_maximum_demand_spring/autumn_workday', 'daily_range_factor_winter_weekend', 'daily_range_factor_winter_workday', 'daily_nonuniformity_coefficient_winter_workday', 'daily_range_factor_spring/autumn_workday', 'daily_load_factor_spring/autumn_workday', 'variance_winter_workday', 'morning_slope_summer_weekend', 'median_daily_maximum_demand_summer_workday', 'median_daily_maximum_demand_winter_workday', 'maximum_tou_spring/autumn_workday', 'morning_slope_winter_weekend', 'variance_winter_weekend', 'pv_correlation_winter_weekend', 'minimum_tou_spring/autumn_workday', 'daily_nonuniformity_coefficient_winter_weekend', 'variance_spring/autumn_weekend', 'daily_load_factor_winter_weekend', 'minimum_tou_winter_workday', 'variance_summer_weekend'] \n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_AFPS_Profit_2020-09-25.pkl')\n",
    "\n",
    "X = data[selected_features]\n",
    "y = data['CommunityProfit_Class']\n",
    "\n",
    "# for col in X.columns:\n",
    "#     X[col] = pd.to_numeric(X[col])\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr_params = {\n",
    "    'penalty' : ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 20, 100],\n",
    "    'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0' : [10e-5, 10e-4, 10e-3, 10e-2, 1],\n",
    "    'tol' : [0.00001, 0.0001, 0.001],\n",
    "    'l1_ratio' : [0.15, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "lr_classifier = SGDClassifier(loss='log',\n",
    "                              random_state=42,\n",
    "                              max_iter=10e+6)\n",
    "\n",
    "lr_gridsearch = GridSearchCV(estimator=lr_classifier,\n",
    "                             param_grid=lr_params,\n",
    "                             scoring='accuracy',\n",
    "                             cv=3,\n",
    "                             n_jobs=3)\n",
    "\n",
    "lr_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "lr_result = pd.DataFrame(lr_gridsearch.cv_results_)\n",
    "lr_result.to_pickle('Data/GridSearchResults_AFPS_Profit_SelectedFeatures_LR.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=lr_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1, 'eta0': 0.01, 'l1_ratio': 0.3, 'learning_rate': 'adaptive', 'penalty': 'elasticnet', 'tol': 0.001}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_AFPS_Profit_SelectedFeatures_LR.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme Gradient Boosting (AFPS / Profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features (AFPS / Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9360 candidates, totalling 28080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=3)]: Done 109 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=3)]: Done 283 tasks      | elapsed:   18.3s\n",
      "[Parallel(n_jobs=3)]: Done 509 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=3)]: Done 782 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done 946 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=3)]: Done 1133 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=3)]: Done 1346 tasks      | elapsed: 26.0min\n",
      "[Parallel(n_jobs=3)]: Done 1654 tasks      | elapsed: 34.7min\n",
      "[Parallel(n_jobs=3)]: Done 2180 tasks      | elapsed: 35.5min\n",
      "[Parallel(n_jobs=3)]: Done 2469 tasks      | elapsed: 38.6min\n",
      "[Parallel(n_jobs=3)]: Done 2782 tasks      | elapsed: 66.0min\n",
      "[Parallel(n_jobs=3)]: Done 3325 tasks      | elapsed: 68.7min\n",
      "[Parallel(n_jobs=3)]: Done 3821 tasks      | elapsed: 71.0min\n",
      "[Parallel(n_jobs=3)]: Done 4208 tasks      | elapsed: 100.5min\n",
      "[Parallel(n_jobs=3)]: Done 4925 tasks      | elapsed: 101.4min\n",
      "[Parallel(n_jobs=3)]: Done 5397 tasks      | elapsed: 108.4min\n",
      "[Parallel(n_jobs=3)]: Done 5891 tasks      | elapsed: 133.3min\n",
      "[Parallel(n_jobs=3)]: Done 6629 tasks      | elapsed: 135.9min\n",
      "[Parallel(n_jobs=3)]: Done 7159 tasks      | elapsed: 163.8min\n",
      "[Parallel(n_jobs=3)]: Done 8015 tasks      | elapsed: 166.3min\n",
      "[Parallel(n_jobs=3)]: Done 8627 tasks      | elapsed: 196.6min\n",
      "[Parallel(n_jobs=3)]: Done 9504 tasks      | elapsed: 200.5min\n",
      "[Parallel(n_jobs=3)]: Done 10193 tasks      | elapsed: 230.5min\n",
      "[Parallel(n_jobs=3)]: Done 11036 tasks      | elapsed: 239.8min\n",
      "[Parallel(n_jobs=3)]: Done 12035 tasks      | elapsed: 261.9min\n",
      "[Parallel(n_jobs=3)]: Done 12722 tasks      | elapsed: 283.7min\n",
      "[Parallel(n_jobs=3)]: Done 13837 tasks      | elapsed: 291.6min\n",
      "[Parallel(n_jobs=3)]: Done 14910 tasks      | elapsed: 308.2min\n",
      "[Parallel(n_jobs=3)]: Done 15797 tasks      | elapsed: 330.3min\n",
      "[Parallel(n_jobs=3)]: Done 16799 tasks      | elapsed: 371.6min\n",
      "[Parallel(n_jobs=3)]: Done 17945 tasks      | elapsed: 380.7min\n",
      "[Parallel(n_jobs=3)]: Done 19091 tasks      | elapsed: 399.9min\n",
      "[Parallel(n_jobs=3)]: Done 20141 tasks      | elapsed: 421.8min\n",
      "[Parallel(n_jobs=3)]: Done 21174 tasks      | elapsed: 443.9min\n",
      "[Parallel(n_jobs=3)]: Done 22369 tasks      | elapsed: 456.7min\n",
      "[Parallel(n_jobs=3)]: Done 23588 tasks      | elapsed: 470.0min\n",
      "[Parallel(n_jobs=3)]: Done 24887 tasks      | elapsed: 484.9min\n",
      "[Parallel(n_jobs=3)]: Done 26210 tasks      | elapsed: 500.6min\n",
      "[Parallel(n_jobs=3)]: Done 27661 tasks      | elapsed: 517.3min\n",
      "[Parallel(n_jobs=3)]: Done 28080 out of 28080 | elapsed: 531.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 5.3e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.38      0.37      0.38       100\n",
      "         Low       0.35      0.31      0.33       100\n",
      "      Medium       0.37      0.42      0.39       100\n",
      "\n",
      "    accuracy                           0.37       300\n",
      "   macro avg       0.37      0.37      0.37       300\n",
      "weighted avg       0.37      0.37      0.37       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_AFPS_Profit_2020-09-25.pkl')\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "y = data['CommunityProfit_Class']\n",
    "\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col])\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_params = {\n",
    "    'reg_alpha' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'reg_lambda' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'learning_rate' : [0, 0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth ' : [3, 5, 7, 9],\n",
    "    'n_estimators' : [10,11,12,13,14,15,20,50,100,200,500,1000,2000]\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax',\n",
    "                                   eval_metric='merror',\n",
    "                                   random_state=42)\n",
    "\n",
    "xgb_gridsearch = GridSearchCV(estimator=xgb_classifier,\n",
    "                              param_grid=xgb_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=3,\n",
    "                              verbose=6)\n",
    "\n",
    "xgb_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "xgb_result = pd.DataFrame(xgb_gridsearch.cv_results_)\n",
    "xgb_result.to_pickle('Data/GridSearchResults_AFPS_Profit_AllFeatures_XGB.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=xgb_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.5, 'max_depth ': 3, 'n_estimators': 10, 'reg_alpha': 0, 'reg_lambda': 0}\n",
      "{'learning_rate': 0.5, 'max_depth ': 3, 'n_estimators': 10, 'reg_alpha': 0, 'reg_lambda': 0.001}\n",
      "{'learning_rate': 0.5, 'max_depth ': 3, 'n_estimators': 10, 'reg_alpha': 0.001, 'reg_lambda': 0}\n",
      "{'learning_rate': 0.5, 'max_depth ': 5, 'n_estimators': 10, 'reg_alpha': 0, 'reg_lambda': 0}\n",
      "{'learning_rate': 0.5, 'max_depth ': 5, 'n_estimators': 10, 'reg_alpha': 0, 'reg_lambda': 0.001}\n",
      "{'learning_rate': 0.5, 'max_depth ': 5, 'n_estimators': 10, 'reg_alpha': 0.001, 'reg_lambda': 0}\n",
      "{'learning_rate': 0.5, 'max_depth ': 7, 'n_estimators': 10, 'reg_alpha': 0, 'reg_lambda': 0}\n",
      "{'learning_rate': 0.5, 'max_depth ': 7, 'n_estimators': 10, 'reg_alpha': 0, 'reg_lambda': 0.001}\n",
      "{'learning_rate': 0.5, 'max_depth ': 7, 'n_estimators': 10, 'reg_alpha': 0.001, 'reg_lambda': 0}\n",
      "{'learning_rate': 0.5, 'max_depth ': 9, 'n_estimators': 10, 'reg_alpha': 0, 'reg_lambda': 0}\n",
      "{'learning_rate': 0.5, 'max_depth ': 9, 'n_estimators': 10, 'reg_alpha': 0, 'reg_lambda': 0.001}\n",
      "{'learning_rate': 0.5, 'max_depth ': 9, 'n_estimators': 10, 'reg_alpha': 0.001, 'reg_lambda': 0}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_AFPS_Profit_AllFeatures_XGB.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selected Features (AFPS / Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9360 candidates, totalling 28080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=3)]: Done  86 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=3)]: Done 338 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=3)]: Done 686 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=3)]: Done 964 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=3)]: Done 1101 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done 1264 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=3)]: Done 1499 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=3)]: Done 2327 tasks      | elapsed:  8.8min\n",
      "[Parallel(n_jobs=3)]: Done 2564 tasks      | elapsed: 10.2min\n",
      "[Parallel(n_jobs=3)]: Done 2827 tasks      | elapsed: 16.7min\n",
      "[Parallel(n_jobs=3)]: Done 3768 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=3)]: Done 4081 tasks      | elapsed: 20.6min\n",
      "[Parallel(n_jobs=3)]: Done 5102 tasks      | elapsed: 25.5min\n",
      "[Parallel(n_jobs=3)]: Done 5465 tasks      | elapsed: 28.6min\n",
      "[Parallel(n_jobs=3)]: Done 6467 tasks      | elapsed: 33.8min\n",
      "[Parallel(n_jobs=3)]: Done 6919 tasks      | elapsed: 38.0min\n",
      "[Parallel(n_jobs=3)]: Done 7983 tasks      | elapsed: 42.2min\n",
      "[Parallel(n_jobs=3)]: Done 8446 tasks      | elapsed: 49.6min\n",
      "[Parallel(n_jobs=3)]: Done 9560 tasks      | elapsed: 51.3min\n",
      "[Parallel(n_jobs=3)]: Done 10697 tasks      | elapsed: 58.0min\n",
      "[Parallel(n_jobs=3)]: Done 11264 tasks      | elapsed: 65.6min\n",
      "[Parallel(n_jobs=3)]: Done 12481 tasks      | elapsed: 68.8min\n",
      "[Parallel(n_jobs=3)]: Done 13722 tasks      | elapsed: 73.4min\n",
      "[Parallel(n_jobs=3)]: Done 14989 tasks      | elapsed: 79.6min\n",
      "[Parallel(n_jobs=3)]: Done 16079 tasks      | elapsed: 86.0min\n",
      "[Parallel(n_jobs=3)]: Done 17135 tasks      | elapsed: 92.7min\n",
      "[Parallel(n_jobs=3)]: Done 18293 tasks      | elapsed: 98.4min\n",
      "[Parallel(n_jobs=3)]: Done 19651 tasks      | elapsed: 104.3min\n",
      "[Parallel(n_jobs=3)]: Done 21033 tasks      | elapsed: 109.9min\n",
      "[Parallel(n_jobs=3)]: Done 22459 tasks      | elapsed: 116.2min\n",
      "[Parallel(n_jobs=3)]: Done 23914 tasks      | elapsed: 121.6min\n",
      "[Parallel(n_jobs=3)]: Done 25648 tasks      | elapsed: 127.3min\n",
      "[Parallel(n_jobs=3)]: Done 27611 tasks      | elapsed: 133.5min\n",
      "[Parallel(n_jobs=3)]: Done 28080 out of 28080 | elapsed: 138.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 1.4e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.29      0.27      0.28       100\n",
      "         Low       0.28      0.14      0.19       100\n",
      "      Medium       0.34      0.53      0.41       100\n",
      "\n",
      "    accuracy                           0.31       300\n",
      "   macro avg       0.30      0.31      0.29       300\n",
      "weighted avg       0.30      0.31      0.29       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Import data (and filter) data\n",
    "selected_features =  ['median_daily_maximum_demand_spring/autumn_workday', 'daily_range_factor_winter_weekend', 'daily_range_factor_winter_workday', 'daily_nonuniformity_coefficient_winter_workday', 'daily_range_factor_spring/autumn_workday', 'daily_load_factor_spring/autumn_workday', 'variance_winter_workday', 'morning_slope_summer_weekend', 'median_daily_maximum_demand_summer_workday', 'median_daily_maximum_demand_winter_workday', 'maximum_tou_spring/autumn_workday', 'morning_slope_winter_weekend', 'variance_winter_weekend', 'pv_correlation_winter_weekend', 'minimum_tou_spring/autumn_workday', 'daily_nonuniformity_coefficient_winter_weekend', 'variance_spring/autumn_weekend', 'daily_load_factor_winter_weekend', 'minimum_tou_winter_workday', 'variance_summer_weekend'] \n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_AFPS_Profit_2020-09-25.pkl')\n",
    "\n",
    "X = data[selected_features]\n",
    "y = data['CommunityProfit_Class']\n",
    "\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col])\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_params = {\n",
    "    'reg_alpha' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'reg_lambda' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'learning_rate' : [0, 0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth ' : [3, 5, 7, 9],\n",
    "    'n_estimators' : [10,11,12,13,14,15,20,50,100,200,500,1000,2000]\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax',\n",
    "                                   eval_metric='merror',\n",
    "                                   random_state=42)\n",
    "\n",
    "xgb_gridsearch = GridSearchCV(estimator=xgb_classifier,\n",
    "                              param_grid=xgb_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=3,\n",
    "                              verbose=6)\n",
    "\n",
    "xgb_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "xgb_result = pd.DataFrame(xgb_gridsearch.cv_results_)\n",
    "xgb_result.to_pickle('Data/GridSearchResults_AFPS_Profit_SelectedFeatures_XGB.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=xgb_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'max_depth ': 3, 'n_estimators': 50, 'reg_alpha': 0.1, 'reg_lambda': 10}\n",
      "{'learning_rate': 0.01, 'max_depth ': 5, 'n_estimators': 50, 'reg_alpha': 0.1, 'reg_lambda': 10}\n",
      "{'learning_rate': 0.01, 'max_depth ': 7, 'n_estimators': 50, 'reg_alpha': 0.1, 'reg_lambda': 10}\n",
      "{'learning_rate': 0.01, 'max_depth ': 9, 'n_estimators': 50, 'reg_alpha': 0.1, 'reg_lambda': 10}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_AFPS_Profit_SelectedFeatures_XGB.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron (AFPS / Profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features (AFPS / Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3000 candidates, totalling 9000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=3)]: Done 104 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=3)]: Done 196 tasks      | elapsed:   25.4s\n",
      "[Parallel(n_jobs=3)]: Done 343 tasks      | elapsed:   43.4s\n",
      "[Parallel(n_jobs=3)]: Done 520 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=3)]: Done 689 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=3)]: Done 880 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=3)]: Done 1144 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=3)]: Done 1441 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=3)]: Done 1721 tasks      | elapsed:  7.5min\n",
      "[Parallel(n_jobs=3)]: Done 2042 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=3)]: Done 2370 tasks      | elapsed: 11.7min\n",
      "[Parallel(n_jobs=3)]: Done 2683 tasks      | elapsed: 22.2min\n",
      "[Parallel(n_jobs=3)]: Done 3029 tasks      | elapsed: 26.6min\n",
      "[Parallel(n_jobs=3)]: Done 3413 tasks      | elapsed: 31.8min\n",
      "[Parallel(n_jobs=3)]: Done 3818 tasks      | elapsed: 42.6min\n",
      "[Parallel(n_jobs=3)]: Done 4231 tasks      | elapsed: 48.4min\n",
      "[Parallel(n_jobs=3)]: Done 4701 tasks      | elapsed: 62.1min\n",
      "[Parallel(n_jobs=3)]: Done 5164 tasks      | elapsed: 70.9min\n",
      "[Parallel(n_jobs=3)]: Done 5651 tasks      | elapsed: 87.0min\n",
      "[Parallel(n_jobs=3)]: Done 6164 tasks      | elapsed: 116.5min\n",
      "[Parallel(n_jobs=3)]: Done 6701 tasks      | elapsed: 146.6min\n",
      "[Parallel(n_jobs=3)]: Done 7264 tasks      | elapsed: 193.8min\n",
      "[Parallel(n_jobs=3)]: Done 7851 tasks      | elapsed: 239.2min\n",
      "[Parallel(n_jobs=3)]: Done 8464 tasks      | elapsed: 309.8min\n",
      "[Parallel(n_jobs=3)]: Done 9000 out of 9000 | elapsed: 401.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 4e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.34      0.30      0.32       100\n",
      "         Low       0.32      0.37      0.35       100\n",
      "      Medium       0.37      0.37      0.37       100\n",
      "\n",
      "    accuracy                           0.35       300\n",
      "   macro avg       0.35      0.35      0.35       300\n",
      "weighted avg       0.35      0.35      0.35       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_AFPS_Profit_2020-09-25.pkl')\n",
    "\n",
    "X = StandardScaler().fit_transform(data.iloc[:,:-1])\n",
    "y = data['CommunityProfit_Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "hls = []\n",
    "for i in [10, 30, 50, 70, 100]:\n",
    "    for j in [1, 2, 3, 4 , 5, 10, 30, 50, 70, 100]:\n",
    "        hls.append((i,)*j)\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes' : hls,\n",
    "    'solver' : ['adam', 'lbfgs'],\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'beta_1' : [0.8, 0.9],\n",
    "    'tol' : [0.000001, 0.00001, 0.0001]\n",
    "}\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42,\n",
    "                               max_iter=10e+8,\n",
    "                               activation='relu')\n",
    "\n",
    "mlp_gridsearch = GridSearchCV(estimator=mlp_classifier,\n",
    "                              param_grid=mlp_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=3,\n",
    "                              verbose=6)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mlp_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "mlp_result = pd.DataFrame(mlp_gridsearch.cv_results_)\n",
    "mlp_result.to_pickle('Data/GridSearchResults_AFPS_Profit_AllFeatures_MLP.pkl')\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=mlp_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.01, 'beta_1': 0.8, 'hidden_layer_sizes': (30, 30, 30, 30, 30, 30, 30, 30, 30, 30), 'solver': 'lbfgs', 'tol': 0.0001}\n",
      "{'alpha': 0.01, 'beta_1': 0.9, 'hidden_layer_sizes': (30, 30, 30, 30, 30, 30, 30, 30, 30, 30), 'solver': 'lbfgs', 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_AFPS_Profit_AllFeatures_MLP.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Python modules\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from datetime import datetime\n",
    "# import time\n",
    "\n",
    "# data = pd.read_pickle('Data/Parameter_Values_Aggregated_Profit_2020-09-15.pkl')\n",
    "\n",
    "# X = X_train = StandardScaler().fit_transform(data.iloc[:,:94])\n",
    "# y = data['CommunityProfit_Class']\n",
    "\n",
    "# # Train-Test-Split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# mlp_classifier = MLPClassifier(hidden_layer_sizes=(50,),\n",
    "#                                activation='relu',\n",
    "#                                solver='adam',\n",
    "#                                random_state=42,\n",
    "#                                max_iter=1000000,\n",
    "#                                beta_1=0.8,\n",
    "#                                verbose=0,\n",
    "#                                tol=0.00001)\n",
    "\n",
    "# mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# print(classification_report(y_true=y_test,\n",
    "#                             y_pred=mlp_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selected Features (AFPS / Profit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3000 candidates, totalling 9000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   8 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=3)]: Done  68 tasks      | elapsed:   34.2s\n",
      "[Parallel(n_jobs=3)]: Done 131 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=3)]: Done 290 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=3)]: Done 416 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=3)]: Done 567 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=3)]: Done 745 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=3)]: Done 932 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=3)]: Done 1175 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=3)]: Done 1412 tasks      | elapsed: 15.7min\n",
      "[Parallel(n_jobs=3)]: Done 1675 tasks      | elapsed: 19.8min\n",
      "[Parallel(n_jobs=3)]: Done 1968 tasks      | elapsed: 40.9min\n",
      "[Parallel(n_jobs=3)]: Done 2434 tasks      | elapsed: 55.7min\n",
      "[Parallel(n_jobs=3)]: Done 2771 tasks      | elapsed: 67.1min\n",
      "[Parallel(n_jobs=3)]: Done 3170 tasks      | elapsed: 71.3min\n",
      "[Parallel(n_jobs=3)]: Done 3557 tasks      | elapsed: 81.1min\n",
      "[Parallel(n_jobs=3)]: Done 3970 tasks      | elapsed: 87.0min\n",
      "[Parallel(n_jobs=3)]: Done 4407 tasks      | elapsed: 104.6min\n",
      "[Parallel(n_jobs=3)]: Done 4870 tasks      | elapsed: 120.1min\n",
      "[Parallel(n_jobs=3)]: Done 5357 tasks      | elapsed: 146.6min\n",
      "[Parallel(n_jobs=3)]: Done 5870 tasks      | elapsed: 165.4min\n",
      "[Parallel(n_jobs=3)]: Done 6407 tasks      | elapsed: 216.0min\n",
      "[Parallel(n_jobs=3)]: Done 6970 tasks      | elapsed: 247.2min\n",
      "[Parallel(n_jobs=3)]: Done 7557 tasks      | elapsed: 292.4min\n",
      "[Parallel(n_jobs=3)]: Done 8170 tasks      | elapsed: 366.7min\n",
      "[Parallel(n_jobs=3)]: Done 8807 tasks      | elapsed: 407.9min\n",
      "[Parallel(n_jobs=3)]: Done 9000 out of 9000 | elapsed: 450.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 4.5e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.42      0.49      0.45       100\n",
      "         Low       0.34      0.33      0.34       100\n",
      "      Medium       0.38      0.33      0.35       100\n",
      "\n",
      "    accuracy                           0.38       300\n",
      "   macro avg       0.38      0.38      0.38       300\n",
      "weighted avg       0.38      0.38      0.38       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_AFPS_Profit_2020-09-25.pkl')\n",
    "\n",
    "selected_features =  ['median_daily_maximum_demand_spring/autumn_workday', 'daily_range_factor_winter_weekend', 'daily_range_factor_winter_workday', 'daily_nonuniformity_coefficient_winter_workday', 'daily_range_factor_spring/autumn_workday', 'daily_load_factor_spring/autumn_workday', 'variance_winter_workday', 'morning_slope_summer_weekend', 'median_daily_maximum_demand_summer_workday', 'median_daily_maximum_demand_winter_workday', 'maximum_tou_spring/autumn_workday', 'morning_slope_winter_weekend', 'variance_winter_weekend', 'pv_correlation_winter_weekend', 'minimum_tou_spring/autumn_workday', 'daily_nonuniformity_coefficient_winter_weekend', 'variance_spring/autumn_weekend', 'daily_load_factor_winter_weekend', 'minimum_tou_winter_workday', 'variance_summer_weekend'] \n",
    "\n",
    "X = StandardScaler().fit_transform(data[selected_features])\n",
    "y = data['CommunityProfit_Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "hls = []\n",
    "for i in [10, 30, 50, 70, 100]:\n",
    "    for j in [1, 2, 3, 4 , 5, 10, 30, 50, 70, 100]:\n",
    "        hls.append((i,)*j)\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes' : hls,\n",
    "    'solver' : ['adam', 'lbfgs'],\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'beta_1' : [0.8, 0.9],\n",
    "    'tol' : [0.000001, 0.00001, 0.0001]\n",
    "}\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42,\n",
    "                               max_iter=10e+8,\n",
    "                               activation='relu')\n",
    "\n",
    "mlp_gridsearch = GridSearchCV(estimator=mlp_classifier,\n",
    "                              param_grid=mlp_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=3,\n",
    "                              verbose=6)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mlp_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "mlp_result = pd.DataFrame(mlp_gridsearch.cv_results_)\n",
    "mlp_result.to_pickle('Data/GridSearchResults_AFPS_Profit_SelectedFeatures_MLP.pkl')\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=mlp_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.01, 'beta_1': 0.8, 'hidden_layer_sizes': (10, 10), 'solver': 'lbfgs', 'tol': 1e-06}\n",
      "{'alpha': 0.01, 'beta_1': 0.8, 'hidden_layer_sizes': (10, 10), 'solver': 'lbfgs', 'tol': 1e-05}\n",
      "{'alpha': 0.01, 'beta_1': 0.8, 'hidden_layer_sizes': (10, 10), 'solver': 'lbfgs', 'tol': 0.0001}\n",
      "{'alpha': 0.01, 'beta_1': 0.9, 'hidden_layer_sizes': (10, 10), 'solver': 'lbfgs', 'tol': 1e-06}\n",
      "{'alpha': 0.01, 'beta_1': 0.9, 'hidden_layer_sizes': (10, 10), 'solver': 'lbfgs', 'tol': 1e-05}\n",
      "{'alpha': 0.01, 'beta_1': 0.9, 'hidden_layer_sizes': (10, 10), 'solver': 'lbfgs', 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_AFPS_Profit_SelectedFeatures_MLP.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation-First-Parametrization-Second / Community Gain (AFPS / Gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression (AFPS / Gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features (AFPS / Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 2e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.33      0.31      0.32       100\n",
      "         Low       0.33      0.60      0.43       100\n",
      "      Medium       0.27      0.07      0.11       100\n",
      "\n",
      "    accuracy                           0.33       300\n",
      "   macro avg       0.31      0.33      0.29       300\n",
      "weighted avg       0.31      0.33      0.29       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_AFPS_Gain_2020-09-25.pkl')\n",
    "\n",
    "X = data.iloc[:,:-1].copy()\n",
    "y = data['CommunityGain_Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr_params = {\n",
    "    'penalty' : ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 20, 100],\n",
    "    'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0' : [10e-5, 10e-4, 10e-3, 10e-2, 1],\n",
    "    'tol' : [0.00001, 0.0001, 0.001],\n",
    "    'l1_ratio' : [0.15, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "lr_classifier = SGDClassifier(loss='log',\n",
    "                              random_state=42,\n",
    "                              max_iter=10e+6)\n",
    "\n",
    "lr_gridsearch = GridSearchCV(estimator=lr_classifier,\n",
    "                             param_grid=lr_params,\n",
    "                             scoring='accuracy',\n",
    "                             cv=3,\n",
    "                             n_jobs=3)\n",
    "\n",
    "lr_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "lr_result = pd.DataFrame(lr_gridsearch.cv_results_)\n",
    "lr_result.to_pickle('Data/GridSearchResults_AFPS_Gain_AllFeatures_LR.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=lr_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 20, 'eta0': 0.0001, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 0.0001, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 0.0001, 'l1_ratio': 0.3, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 0.0001, 'l1_ratio': 0.3, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 0.0001, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 0.0001, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 0.001, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 0.001, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 0.001, 'l1_ratio': 0.3, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 0.001, 'l1_ratio': 0.3, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 0.001, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 0.001, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 0.01, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 0.01, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 0.01, 'l1_ratio': 0.3, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 0.01, 'l1_ratio': 0.3, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 0.01, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 0.01, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 0.1, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 0.1, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 0.1, 'l1_ratio': 0.3, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 0.1, 'l1_ratio': 0.3, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 1, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 1, 'l1_ratio': 0.15, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 1, 'l1_ratio': 0.3, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 1, 'l1_ratio': 0.3, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n",
      "{'alpha': 20, 'eta0': 1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 20, 'eta0': 1, 'l1_ratio': 0.5, 'learning_rate': 'optimal', 'penalty': 'l2', 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_AFPS_Gain_AllFeatures_LR.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selected Features (AFPS / Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 1.7e+01 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.35      0.22      0.27       167\n",
      "         Low       0.35      0.50      0.41       167\n",
      "      Medium       0.34      0.32      0.33       166\n",
      "\n",
      "    accuracy                           0.34       500\n",
      "   macro avg       0.34      0.34      0.33       500\n",
      "weighted avg       0.34      0.34      0.33       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "selected_features = ['variance_summer_weekend', 'kurtosis_summer_weekend', 'median_daily_minimum_demand_summer_weekend', 'median_daily_minimum_demand_summer_workday', 'end_of_work_impact_summer_workday', 'daily_nonuniformity_coefficient_summer_weekend', 'pv_correlation_spring/autumn_weekend', 'skewness_summer_weekend', 'lunch_impact_spring/autumn_weekend', 'kurtosis_summer_workday', 'morning_slope_spring/autumn_weekend', 'skewness_winter_workday', 'summer_winter_ratio_weekend', 'skewness_summer_workday', 'variance_summer_workday', 'maximum_tou_winter_workday', 'minimum_tou_spring/autumn_workday', 'minimum_tou_spring/autumn_weekend', 'minimum_tou_winter_weekend', 'kurtosis_winter_workday']\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_AFPS_Gain_2020-09-25.pkl')\n",
    "\n",
    "X = data[selected_features]\n",
    "y = data['CommunityGain_Class']\n",
    "\n",
    "# for col in X.columns:\n",
    "#     X[col] = pd.to_numeric(X[col])\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "lr_params = {\n",
    "    'penalty' : ['l1', 'l2', 'elasticnet'],\n",
    "    'alpha' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 20, 100],\n",
    "    'learning_rate' : ['constant', 'optimal', 'invscaling', 'adaptive'],\n",
    "    'eta0' : [10e-5, 10e-4, 10e-3, 10e-2, 1],\n",
    "    'tol' : [0.00001, 0.0001, 0.001],\n",
    "    'l1_ratio' : [0.15, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "lr_classifier = SGDClassifier(loss='log',\n",
    "                              random_state=42,\n",
    "                              max_iter=10e+6)\n",
    "\n",
    "lr_gridsearch = GridSearchCV(estimator=lr_classifier,\n",
    "                             param_grid=lr_params,\n",
    "                             scoring='accuracy',\n",
    "                             cv=3,\n",
    "                             n_jobs=3)\n",
    "\n",
    "lr_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "lr_result = pd.DataFrame(lr_gridsearch.cv_results_)\n",
    "lr_result.to_pickle('Data/GridSearchResults_AFPS_Gain_SelectedFeatures_LR.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=lr_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.15, 'learning_rate': 'adaptive', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.3, 'learning_rate': 'adaptive', 'penalty': 'l2', 'tol': 1e-05}\n",
      "{'alpha': 0.1, 'eta0': 0.1, 'l1_ratio': 0.5, 'learning_rate': 'adaptive', 'penalty': 'l2', 'tol': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_AFPS_Gain_SelectedFeatures_LR.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extreme Gradient Boosting (AFPS / Gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features (AFPS / Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9360 candidates, totalling 28080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:   17.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   24.9s\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:   36.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1505 tasks      | elapsed: 47.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed: 48.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2105 tasks      | elapsed: 48.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2442 tasks      | elapsed: 51.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2805 tasks      | elapsed: 87.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3192 tasks      | elapsed: 87.8min\n",
      "[Parallel(n_jobs=-1)]: Done 3605 tasks      | elapsed: 89.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4042 tasks      | elapsed: 105.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4505 tasks      | elapsed: 130.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4992 tasks      | elapsed: 131.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5505 tasks      | elapsed: 145.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6042 tasks      | elapsed: 161.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6605 tasks      | elapsed: 163.8min\n",
      "[Parallel(n_jobs=-1)]: Done 7248 tasks      | elapsed: 194.4min\n",
      "[Parallel(n_jobs=-1)]: Done 8137 tasks      | elapsed: 200.3min\n",
      "[Parallel(n_jobs=-1)]: Done 8774 tasks      | elapsed: 227.8min\n",
      "[Parallel(n_jobs=-1)]: Done 9437 tasks      | elapsed: 230.6min\n",
      "[Parallel(n_jobs=-1)]: Done 10124 tasks      | elapsed: 258.3min\n",
      "[Parallel(n_jobs=-1)]: Done 10837 tasks      | elapsed: 260.8min\n",
      "[Parallel(n_jobs=-1)]: Done 11574 tasks      | elapsed: 288.2min\n",
      "[Parallel(n_jobs=-1)]: Done 12337 tasks      | elapsed: 292.5min\n",
      "[Parallel(n_jobs=-1)]: Done 13124 tasks      | elapsed: 310.9min\n",
      "[Parallel(n_jobs=-1)]: Done 13937 tasks      | elapsed: 323.8min\n",
      "[Parallel(n_jobs=-1)]: Done 14774 tasks      | elapsed: 334.2min\n",
      "[Parallel(n_jobs=-1)]: Done 15637 tasks      | elapsed: 359.1min\n",
      "[Parallel(n_jobs=-1)]: Done 16524 tasks      | elapsed: 362.8min\n",
      "[Parallel(n_jobs=-1)]: Done 17437 tasks      | elapsed: 384.0min\n",
      "[Parallel(n_jobs=-1)]: Done 18374 tasks      | elapsed: 404.0min\n",
      "[Parallel(n_jobs=-1)]: Done 19337 tasks      | elapsed: 407.7min\n",
      "[Parallel(n_jobs=-1)]: Done 20324 tasks      | elapsed: 423.9min\n",
      "[Parallel(n_jobs=-1)]: Done 21498 tasks      | elapsed: 441.4min\n",
      "[Parallel(n_jobs=-1)]: Done 22832 tasks      | elapsed: 460.5min\n",
      "[Parallel(n_jobs=-1)]: Done 24101 tasks      | elapsed: 476.5min\n",
      "[Parallel(n_jobs=-1)]: Done 25188 tasks      | elapsed: 489.0min\n",
      "[Parallel(n_jobs=-1)]: Done 26301 tasks      | elapsed: 497.6min\n",
      "[Parallel(n_jobs=-1)]: Done 27438 tasks      | elapsed: 513.3min\n",
      "[Parallel(n_jobs=-1)]: Done 28080 out of 28080 | elapsed: 530.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 5.3e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.31      0.32      0.32       100\n",
      "         Low       0.26      0.24      0.25       100\n",
      "      Medium       0.31      0.33      0.32       100\n",
      "\n",
      "    accuracy                           0.30       300\n",
      "   macro avg       0.30      0.30      0.30       300\n",
      "weighted avg       0.30      0.30      0.30       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_AFPS_Gain_2020-09-25.pkl')\n",
    "\n",
    "X = data.iloc[:,:-1]\n",
    "y = data['CommunityGain_Class']\n",
    "\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col])\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_params = {\n",
    "    'reg_alpha' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'reg_lambda' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'learning_rate' : [0, 0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth ' : [3, 5, 7, 9],\n",
    "    'n_estimators' : [10,11,12,13,14,15,20,50,100,200,500,1000,2000]\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax',\n",
    "                                   eval_metric='merror',\n",
    "                                   random_state=42)\n",
    "\n",
    "xgb_gridsearch = GridSearchCV(estimator=xgb_classifier,\n",
    "                              param_grid=xgb_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=6)\n",
    "\n",
    "xgb_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "xgb_result = pd.DataFrame(xgb_gridsearch.cv_results_)\n",
    "xgb_result.to_pickle('Data/GridSearchResults_AFPS_Gain_AllFeatures_XGB.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=xgb_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.5, 'max_depth ': 3, 'n_estimators': 10, 'reg_alpha': 10, 'reg_lambda': 0.1}\n",
      "{'learning_rate': 0.5, 'max_depth ': 5, 'n_estimators': 10, 'reg_alpha': 10, 'reg_lambda': 0.1}\n",
      "{'learning_rate': 0.5, 'max_depth ': 7, 'n_estimators': 10, 'reg_alpha': 10, 'reg_lambda': 0.1}\n",
      "{'learning_rate': 0.5, 'max_depth ': 9, 'n_estimators': 10, 'reg_alpha': 10, 'reg_lambda': 0.1}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_AFPS_Gain_AllFeatures_XGB.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selected Features (AFPS / Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 9360 candidates, totalling 28080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 170 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 344 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=-1)]: Done 954 tasks      | elapsed:   54.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1117 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1304 tasks      | elapsed:  5.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1668 tasks      | elapsed:  9.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2394 tasks      | elapsed:  9.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2657 tasks      | elapsed: 12.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3152 tasks      | elapsed: 16.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3885 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4222 tasks      | elapsed: 24.3min\n",
      "[Parallel(n_jobs=-1)]: Done 5129 tasks      | elapsed: 24.8min\n",
      "[Parallel(n_jobs=-1)]: Done 5516 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6537 tasks      | elapsed: 32.4min\n",
      "[Parallel(n_jobs=-1)]: Done 6974 tasks      | elapsed: 38.2min\n",
      "[Parallel(n_jobs=-1)]: Done 8041 tasks      | elapsed: 39.9min\n",
      "[Parallel(n_jobs=-1)]: Done 8652 tasks      | elapsed: 46.3min\n",
      "[Parallel(n_jobs=-1)]: Done 9681 tasks      | elapsed: 49.6min\n",
      "[Parallel(n_jobs=-1)]: Done 10858 tasks      | elapsed: 54.2min\n",
      "[Parallel(n_jobs=-1)]: Done 11792 tasks      | elapsed: 60.8min\n",
      "[Parallel(n_jobs=-1)]: Done 12640 tasks      | elapsed: 66.6min\n",
      "[Parallel(n_jobs=-1)]: Done 13861 tasks      | elapsed: 69.1min\n",
      "[Parallel(n_jobs=-1)]: Done 15042 tasks      | elapsed: 73.2min\n",
      "[Parallel(n_jobs=-1)]: Done 16256 tasks      | elapsed: 78.8min\n",
      "[Parallel(n_jobs=-1)]: Done 17380 tasks      | elapsed: 84.6min\n",
      "[Parallel(n_jobs=-1)]: Done 18584 tasks      | elapsed: 90.2min\n",
      "[Parallel(n_jobs=-1)]: Done 19920 tasks      | elapsed: 95.2min\n",
      "[Parallel(n_jobs=-1)]: Done 21312 tasks      | elapsed: 100.2min\n",
      "[Parallel(n_jobs=-1)]: Done 22708 tasks      | elapsed: 105.1min\n",
      "[Parallel(n_jobs=-1)]: Done 24300 tasks      | elapsed: 109.8min\n",
      "[Parallel(n_jobs=-1)]: Done 25968 tasks      | elapsed: 114.5min\n",
      "[Parallel(n_jobs=-1)]: Done 27573 tasks      | elapsed: 119.4min\n",
      "[Parallel(n_jobs=-1)]: Done 28080 out of 28080 | elapsed: 123.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 1.2e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.35      0.35      0.35       100\n",
      "         Low       0.36      0.34      0.35       100\n",
      "      Medium       0.32      0.33      0.32       100\n",
      "\n",
      "    accuracy                           0.34       300\n",
      "   macro avg       0.34      0.34      0.34       300\n",
      "weighted avg       0.34      0.34      0.34       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Import data (and filter) data\n",
    "selected_features = ['variance_summer_weekend', 'kurtosis_summer_weekend', 'median_daily_minimum_demand_summer_weekend', 'median_daily_minimum_demand_summer_workday', 'end_of_work_impact_summer_workday', 'daily_nonuniformity_coefficient_summer_weekend', 'pv_correlation_spring/autumn_weekend', 'skewness_summer_weekend', 'lunch_impact_spring/autumn_weekend', 'kurtosis_summer_workday', 'morning_slope_spring/autumn_weekend', 'skewness_winter_workday', 'summer_winter_ratio_weekend', 'skewness_summer_workday', 'variance_summer_workday', 'maximum_tou_winter_workday', 'minimum_tou_spring/autumn_workday', 'minimum_tou_spring/autumn_weekend', 'minimum_tou_winter_weekend', 'kurtosis_winter_workday']\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_AFPS_Gain_2020-09-25.pkl')\n",
    "\n",
    "X = data[selected_features].copy()\n",
    "y = data['CommunityGain_Class']\n",
    "\n",
    "for col in X.columns:\n",
    "    X[col] = pd.to_numeric(X[col])\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_params = {\n",
    "    'reg_alpha' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'reg_lambda' : [0, 0.001, 0.1, 1, 10, 100],\n",
    "    'learning_rate' : [0, 0.01, 0.1, 0.3, 0.5],\n",
    "    'max_depth ' : [3, 5, 7, 9],\n",
    "    'n_estimators' : [10,11,12,13,14,15,20,50,100,200,500,1000,2000]\n",
    "}\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax',\n",
    "                                   eval_metric='merror',\n",
    "                                   random_state=42)\n",
    "\n",
    "xgb_gridsearch = GridSearchCV(estimator=xgb_classifier,\n",
    "                              param_grid=xgb_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=6)\n",
    "\n",
    "xgb_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "xgb_result = pd.DataFrame(xgb_gridsearch.cv_results_)\n",
    "xgb_result.to_pickle('Data/GridSearchResults_AFPS_Gain_SelectedFeatures_XGB.pkl')\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=xgb_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.5, 'max_depth ': 3, 'n_estimators': 20, 'reg_alpha': 1, 'reg_lambda': 100}\n",
      "{'learning_rate': 0.5, 'max_depth ': 5, 'n_estimators': 20, 'reg_alpha': 1, 'reg_lambda': 100}\n",
      "{'learning_rate': 0.5, 'max_depth ': 7, 'n_estimators': 20, 'reg_alpha': 1, 'reg_lambda': 100}\n",
      "{'learning_rate': 0.5, 'max_depth ': 9, 'n_estimators': 20, 'reg_alpha': 1, 'reg_lambda': 100}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_AFPS_Gain_SelectedFeatures_XGB.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Layer Perceptron (AFPS / Gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All Features (AFPS / Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3000 candidates, totalling 9000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 149 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done 306 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 450 tasks      | elapsed:   36.8s\n",
      "[Parallel(n_jobs=-1)]: Done 601 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 764 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 952 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1232 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 1510 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1773 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=-1)]: Done 2084 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2397 tasks      | elapsed:  9.9min\n",
      "[Parallel(n_jobs=-1)]: Done 2734 tasks      | elapsed: 17.9min\n",
      "[Parallel(n_jobs=-1)]: Done 3097 tasks      | elapsed: 19.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3484 tasks      | elapsed: 24.0min\n",
      "[Parallel(n_jobs=-1)]: Done 3897 tasks      | elapsed: 31.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4334 tasks      | elapsed: 39.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4797 tasks      | elapsed: 48.4min\n",
      "[Parallel(n_jobs=-1)]: Done 5284 tasks      | elapsed: 56.5min\n",
      "[Parallel(n_jobs=-1)]: Done 5797 tasks      | elapsed: 69.8min\n",
      "[Parallel(n_jobs=-1)]: Done 6334 tasks      | elapsed: 103.9min\n",
      "[Parallel(n_jobs=-1)]: Done 6897 tasks      | elapsed: 118.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7484 tasks      | elapsed: 151.7min\n",
      "[Parallel(n_jobs=-1)]: Done 8097 tasks      | elapsed: 217.1min\n",
      "[Parallel(n_jobs=-1)]: Done 8734 tasks      | elapsed: 249.4min\n",
      "[Parallel(n_jobs=-1)]: Done 9000 out of 9000 | elapsed: 295.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation Runtime: 3e+02 Minutes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.28      0.28      0.28       100\n",
      "         Low       0.22      0.20      0.21       100\n",
      "      Medium       0.31      0.33      0.32       100\n",
      "\n",
      "    accuracy                           0.27       300\n",
      "   macro avg       0.27      0.27      0.27       300\n",
      "weighted avg       0.27      0.27      0.27       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_AFPS_Gain_2020-09-25.pkl')\n",
    "\n",
    "X = StandardScaler().fit_transform(data.iloc[:,:-1])\n",
    "y = data['CommunityGain_Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "hls = []\n",
    "for i in [10, 30, 50, 70, 100]:\n",
    "    for j in [1, 2, 3, 4 , 5, 10, 30, 50, 70, 100]:\n",
    "        hls.append((i,)*j)\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes' : hls,\n",
    "    'solver' : ['adam', 'lbfgs'],\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'beta_1' : [0.8, 0.9],\n",
    "    'tol' : [0.000001, 0.00001, 0.0001]\n",
    "}\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42,\n",
    "                               max_iter=10e+8,\n",
    "                               activation='relu')\n",
    "\n",
    "mlp_gridsearch = GridSearchCV(estimator=mlp_classifier,\n",
    "                              param_grid=mlp_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=6)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mlp_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "mlp_result = pd.DataFrame(mlp_gridsearch.cv_results_)\n",
    "mlp_result.to_pickle('Data/GridSearchResults_AFPS_Gain_AllFeatures_MLP.pkl')\n",
    "\n",
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=mlp_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.001, 'beta_1': 0.8, 'hidden_layer_sizes': (100, 100, 100, 100, 100, 100, 100, 100, 100, 100), 'solver': 'lbfgs', 'tol': 1e-05}\n",
      "{'alpha': 0.001, 'beta_1': 0.9, 'hidden_layer_sizes': (100, 100, 100, 100, 100, 100, 100, 100, 100, 100), 'solver': 'lbfgs', 'tol': 1e-05}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_AFPS_Gain_AllFeatures_MLP.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import Python modules\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from datetime import datetime\n",
    "# import time\n",
    "\n",
    "# data = pd.read_pickle('Data/Parameter_Values_Aggregated_Profit_2020-09-15.pkl')\n",
    "\n",
    "# X = X_train = StandardScaler().fit_transform(data.iloc[:,:94])\n",
    "# y = data['CommunityProfit_Class']\n",
    "\n",
    "# # Train-Test-Split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# mlp_classifier = MLPClassifier(hidden_layer_sizes=(50,),\n",
    "#                                activation='relu',\n",
    "#                                solver='adam',\n",
    "#                                random_state=42,\n",
    "#                                max_iter=1000000,\n",
    "#                                beta_1=0.8,\n",
    "#                                verbose=0,\n",
    "#                                tol=0.00001)\n",
    "\n",
    "# mlp_classifier.fit(X_train, y_train)\n",
    "\n",
    "# print(classification_report(y_true=y_test,\n",
    "#                             y_pred=mlp_classifier.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selected Features (AFPS / Gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "selected_features = ['variance_summer_weekend', 'kurtosis_summer_weekend', 'median_daily_minimum_demand_summer_weekend', 'median_daily_minimum_demand_summer_workday', 'end_of_work_impact_summer_workday', 'daily_nonuniformity_coefficient_summer_weekend', 'pv_correlation_spring/autumn_weekend', 'skewness_summer_weekend', 'lunch_impact_spring/autumn_weekend', 'kurtosis_summer_workday', 'morning_slope_spring/autumn_weekend', 'skewness_winter_workday', 'summer_winter_ratio_weekend', 'skewness_summer_workday', 'variance_summer_workday', 'maximum_tou_winter_workday', 'minimum_tou_spring/autumn_workday', 'minimum_tou_spring/autumn_weekend', 'minimum_tou_winter_weekend', 'kurtosis_winter_workday']\n",
    "\n",
    "data = pd.read_pickle('Data/Parameter_Values_AFPS_Gain_2020-09-25.pkl')\n",
    "\n",
    "X = StandardScaler().fit_transform(data[selected_features])\n",
    "y = data['CommunityGain_Class']\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "n\n",
    "hls = []\n",
    "for i in [10, 30, 50, 70, 100]:\n",
    "    for j in [1, 2, 3, 4 , 5, 10, 30, 50, 70, 100]:\n",
    "        hls.append((i,)*j)\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes' : hls,\n",
    "    'solver' : ['adam', 'lbfgs'],\n",
    "    'alpha' : [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'beta_1' : [0.8, 0.9],\n",
    "    'tol' : [0.000001, 0.00001, 0.0001]\n",
    "}\n",
    "\n",
    "mlp_classifier = MLPClassifier(random_state=42,\n",
    "                               max_iter=10e+8,\n",
    "                               activation='relu')\n",
    "\n",
    "mlp_gridsearch = GridSearchCV(estimator=mlp_classifier,\n",
    "                              param_grid=mlp_params,\n",
    "                              scoring='accuracy',\n",
    "                              cv=3,\n",
    "                              n_jobs=-1,\n",
    "                              verbose=6)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mlp_gridsearch.fit(X_train, y_train)\n",
    "\n",
    "print('Calculation Runtime: {:.2} Minutes'.format((time.time() - start_time)/60))\n",
    "\n",
    "mlp_result = pd.DataFrame(mlp_gridsearch.cv_results_)\n",
    "mlp_result.to_pickle('Data/GridSearchResults_AFPS_Gain_SelectedFeatures_MLP.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.29      0.14      0.19       100\n",
      "         Low       0.29      0.31      0.30       100\n",
      "      Medium       0.31      0.44      0.36       100\n",
      "\n",
      "    accuracy                           0.30       300\n",
      "   macro avg       0.29      0.30      0.28       300\n",
      "weighted avg       0.29      0.30      0.28       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true=y_test,\n",
    "                            y_pred=mlp_gridsearch.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.01, 'beta_1': 0.8, 'hidden_layer_sizes': (10, 10, 10, 10, 10), 'solver': 'lbfgs', 'tol': 1e-06}\n",
      "{'alpha': 0.01, 'beta_1': 0.8, 'hidden_layer_sizes': (10, 10, 10, 10, 10), 'solver': 'lbfgs', 'tol': 1e-05}\n",
      "{'alpha': 0.01, 'beta_1': 0.8, 'hidden_layer_sizes': (10, 10, 10, 10, 10), 'solver': 'lbfgs', 'tol': 0.0001}\n",
      "{'alpha': 0.01, 'beta_1': 0.9, 'hidden_layer_sizes': (10, 10, 10, 10, 10), 'solver': 'lbfgs', 'tol': 1e-06}\n",
      "{'alpha': 0.01, 'beta_1': 0.9, 'hidden_layer_sizes': (10, 10, 10, 10, 10), 'solver': 'lbfgs', 'tol': 1e-05}\n",
      "{'alpha': 0.01, 'beta_1': 0.9, 'hidden_layer_sizes': (10, 10, 10, 10, 10), 'solver': 'lbfgs', 'tol': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_pickle('Data/GridSearchResults_AFPS_Gain_SelectedFeatures_MLP.pkl')\n",
    "best = result[result['rank_test_score']==1]['params'].to_list()\n",
    "for x in best:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------\n",
    "#### Calculation of the Number of Fitted Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Evaluated Models:  137760\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lr = pd.read_pickle('Data/GridSearchResults_PFAS_Profit_AllFeatures_LR.pkl')\n",
    "xgb = pd.read_pickle('Data/GridSearchResults_PFAS_Profit_AllFeatures_XGB.pkl')\n",
    "mlp = pd.read_pickle('Data/GridSearchResults_PFAS_Profit_AllFeatures_MLP.pkl')\n",
    "\n",
    "n_classification_tasks = 2\n",
    "n_parameter_subsets = 2\n",
    "n_calculation_approches = 2\n",
    "n_models_lr = len(lr.index)\n",
    "n_models_xgb = len(xgb.index)\n",
    "n_models_mlp = len(mlp.index)\n",
    "\n",
    "print('Total Number of Evaluated Models: ', n_classification_tasks*\n",
    "                                            n_parameter_subsets*\n",
    "                                            n_calculation_approches*\n",
    "                                            (n_models_lr + n_models_xgb + n_models_mlp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
